{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: snowflake-connector-python in /opt/conda/lib/python3.12/site-packages (3.17.3)\n",
      "Requirement already satisfied: asn1crypto<2.0.0,>0.24.0 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (1.5.1)\n",
      "Requirement already satisfied: boto3>=1.24 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (1.40.25)\n",
      "Requirement already satisfied: botocore>=1.24 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (1.40.25)\n",
      "Requirement already satisfied: cffi<2.0.0,>=1.9 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (1.17.1)\n",
      "Requirement already satisfied: cryptography>=3.1.0 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (45.0.4)\n",
      "Requirement already satisfied: pyOpenSSL<26.0.0,>=22.0.0 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (25.1.0)\n",
      "Requirement already satisfied: pyjwt<3.0.0 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (2.10.1)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (2025.2)\n",
      "Requirement already satisfied: requests<3.0.0 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (2.32.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (25.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (2025.6.15)\n",
      "Requirement already satisfied: typing_extensions<5,>=4.3 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (4.14.0)\n",
      "Requirement already satisfied: filelock<4,>=3.5 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (3.19.1)\n",
      "Requirement already satisfied: sortedcontainers>=2.4.0 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (2.4.0)\n",
      "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (4.3.8)\n",
      "Requirement already satisfied: tomlkit in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (0.13.3)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.12/site-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python) (2.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3.0.0->snowflake-connector-python) (2.4.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from boto3>=1.24->snowflake-connector-python) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /opt/conda/lib/python3.12/site-packages (from boto3>=1.24->snowflake-connector-python) (0.13.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.12/site-packages (from botocore>=1.24->snowflake-connector-python) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.24->snowflake-connector-python) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade snowflake-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = snowflake.connector.connect(\n",
    "    user= 'mnonog',\n",
    "    password = 'KayaNatinToL0RD!',\n",
    "    account='cejmwpt-djb91267'\n",
    "    )\n",
    "\n",
    "#https://cejmwpt-djb91267.snowflakecomputing.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with conn.cursor() as cs:\n",
    "    # warehouse + db/schema setup\n",
    "    cs.execute(\"CREATE WAREHOUSE IF NOT EXISTS my_first_warehouse\")\n",
    "    cs.execute(\"CREATE DATABASE IF NOT EXISTS testdb\")\n",
    "    cs.execute(\"USE DATABASE testdb\")          # 👈 set current DB\n",
    "    cs.execute(\"CREATE SCHEMA IF NOT EXISTS testschema\")\n",
    "    cs.execute( \"CREATE OR REPLACE TABLE \"    \n",
    "             \"test_table(col1 integer, col2 string)\")\n",
    "    cs.execute(\"INSERT INTO test_table(col1, col2) \"\n",
    "    \t\t\"VALUES (123, 'test string1'), (456, 'test string2')\")\n",
    "    \n",
    "    #  stages \n",
    "    cs.execute(\"CREATE STAGE IF NOT EXISTS purchases_stage\")\n",
    "    cs.execute(\"CREATE STAGE IF NOT EXISTS invoices_stage\")\n",
    "    cs.execute(\"CREATE STAGE IF NOT EXISTS supplier_stage\")\n",
    "    \n",
    "    # File formats\n",
    "    cs.execute(\"\"\"\n",
    "        CREATE OR REPLACE FILE FORMAT csv_ff \n",
    "        TYPE = CSV \n",
    "        FIELD_OPTIONALLY_ENCLOSED_BY = '\"'\n",
    "        SKIP_HEADER = 1 \n",
    "        NULL_IF = ('', 'NULL')\n",
    "    \"\"\")\n",
    "    \n",
    "    cs.execute(\"\"\"\n",
    "        CREATE OR REPLACE FILE FORMAT xml_ff \n",
    "        TYPE = XML \n",
    "        STRIP_OUTER_ELEMENT = TRUE\n",
    "    \"\"\")\n",
    "    # cs.execute('SELECT * FROM test_table')\n",
    "    # print(cs.fetchmany(2))\n",
    "conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2-binary in /opt/conda/lib/python3.12/site-packages (2.9.10)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.12/site-packages (6.0.1)\n",
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.12/site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install psycopg2-binary pandas lxml python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_93494/868590347.py:75: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/868590347.py:75: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/868590347.py:75: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/868590347.py:75: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/868590347.py:75: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/868590347.py:75: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/868590347.py:75: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/868590347.py:75: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/868590347.py:75: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/868590347.py:75: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/868590347.py:75: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/868590347.py:75: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/868590347.py:75: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/868590347.py:75: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/868590347.py:75: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/868590347.py:75: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/868590347.py:75: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/868590347.py:75: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/868590347.py:75: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/868590347.py:75: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/868590347.py:75: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/868590347.py:75: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/868590347.py:75: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/868590347.py:75: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/868590347.py:75: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/868590347.py:75: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/868590347.py:75: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/868590347.py:75: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/868590347.py:75: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/868590347.py:75: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/868590347.py:75: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/868590347.py:75: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/868590347.py:75: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/868590347.py:75: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/868590347.py:75: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/868590347.py:75: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/868590347.py:75: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/868590347.py:75: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/868590347.py:75: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/868590347.py:75: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/868590347.py:75: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combine to 1：/home/jovyan/Desktop/SQL/SQL_FINAL_PROJECT/Data-5/Monthly PO Data/combined_purchases.csv line=245,490\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import snowflake.connector as sf\n",
    "\n",
    "\n",
    "WH, DB, SC = \"ETL_WH\", \"ETL_DB\", \"ETL_SCHEMA\"\n",
    "BASE_DIR = Path(r\"/home/jovyan/Desktop/SQL/SQL_FINAL_PROJECT/Data-5/Monthly PO Data\").resolve()\n",
    "OUT_CSV  = BASE_DIR / \"combined_purchases.csv\"   # 统一输出文件\n",
    "\n",
    "# 2) 需要保留并标准化到 Snowflake 的列（目标列名）\n",
    "TARGET_COLS = [\n",
    "    \"PurchaseOrderID\",\n",
    "    \"PurchaseOrderLineID\",\n",
    "    \"ReceivedOuters\",\n",
    "    \"ExpectedUnitPricePerOuter\",\n",
    "    \"OrderDate\",\n",
    "    \"SupplierID\",\n",
    "]\n",
    "\n",
    "# 3) 列名别名表（尽量覆盖你文件里的各种写法；不够的话再加几种）\n",
    "ALIASES = {\n",
    "    \"PurchaseOrderID\":           [\"purchaseorderid\", \"purchase_order_id\", \"poid\", \"orderid\"],\n",
    "    \"PurchaseOrderLineID\":       [\"purchaseorderlineid\", \"purchase_order_line_id\", \"polineid\", \"orderlineid\"],\n",
    "    \"ReceivedOuters\":            [\"receivedouters\", \"received_outers\", \"receivedoutersqty\", \"receivedoutersquantity\", \"received_qty\"],\n",
    "    \"ExpectedUnitPricePerOuter\": [\"expectedunitpriceperouter\", \"expected_unit_price_per_outer\", \"unitpriceperouter\", \"expectedprice\", \"unitprice\"],\n",
    "    \"OrderDate\":                 [\"orderdate\", \"order_date\", \"date\", \"order_dt\"],\n",
    "    \"SupplierID\":                [\"supplierid\", \"supplier_id\", \"vendorid\", \"vendor_id\"],\n",
    "}\n",
    "\n",
    "def norm(name: str) -> str:\n",
    "    \"\"\"规范化列名：只保留字母数字并小写，便于匹配\"\"\"\n",
    "    return \"\".join(ch.lower() for ch in name if ch.isalnum())\n",
    "\n",
    "def pick_and_rename_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"从原始 df 里按别名表挑出需要的列，并重命名为标准列名\"\"\"\n",
    "    orig_to_norm = {c: norm(c) for c in df.columns}\n",
    "    norm_to_orig = {v: k for k, v in orig_to_norm.items()}  # 取第一个出现的映射即可\n",
    "\n",
    "    selected = {}\n",
    "    missing  = []\n",
    "    for tgt, alias_list in ALIASES.items():\n",
    "        found = None\n",
    "        for alias in alias_list:\n",
    "            if alias in norm_to_orig:\n",
    "                found = norm_to_orig[alias]\n",
    "                break\n",
    "        if found is None:\n",
    "            missing.append(tgt)\n",
    "        else:\n",
    "            selected[tgt] = found\n",
    "\n",
    "    if missing:\n",
    "        raise ValueError(f\"缺少必须列：{missing}; 文件列={list(df.columns)}\")\n",
    "\n",
    "\n",
    "    out = df[[selected[c] for c in TARGET_COLS]].copy()\n",
    "    out.columns = TARGET_COLS\n",
    "    return out\n",
    "\n",
    "frames = []\n",
    "csv_files = sorted(BASE_DIR.glob(\"*.csv\"))\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"在 {BASE_DIR} 下没有找到 .csv 文件\")\n",
    "\n",
    "for p in csv_files:\n",
    "    df = pd.read_csv(p, dtype=str, keep_default_na=False, na_values=[\"\", \"NULL\"])\n",
    "    df_std = pick_and_rename_columns(df)\n",
    "\n",
    "\n",
    "    for c in [\"ReceivedOuters\", \"ExpectedUnitPricePerOuter\"]:\n",
    "        df_std[c] = pd.to_numeric(df_std[c], errors=\"coerce\")\n",
    "\n",
    "    df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
    "\n",
    "\n",
    "    df_std = df_std.dropna(subset=[\"PurchaseOrderID\", \"PurchaseOrderLineID\", \"OrderDate\"])\n",
    "\n",
    "    df_std[\"ReceivedOuters\"] = df_std[\"ReceivedOuters\"].fillna(0)\n",
    "    df_std[\"ExpectedUnitPricePerOuter\"] = df_std[\"ExpectedUnitPricePerOuter\"].fillna(0)\n",
    "\n",
    "    frames.append(df_std)\n",
    "\n",
    "combined = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "combined.to_csv(OUT_CSV, index=False)\n",
    "print(f\"combine to 1：{OUT_CSV} line={len(combined):,}\")\n",
    "\n",
    "conn = sf.connect(\n",
    "    user=os.getenv(\"SNOW_USER\", \"ETHANAN2000\"),\n",
    "    password=os.getenv(\"SNOW_PASSWORD\", \"An67087833@123\"),\n",
    "    account=os.getenv(\"SNOW_ACCOUNT\", \"svogymj-bxb71103\") \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0xffffabd6fb30>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs = conn.cursor()\n",
    "\n",
    "WH, DB, SC = \"ETL_WH\", \"ETL_DB\", \"ETL_SCHEMA\"\n",
    "cs.execute(f\"CREATE WAREHOUSE IF NOT EXISTS {WH} WAREHOUSE_SIZE=SMALL AUTO_SUSPEND=60 AUTO_RESUME=TRUE\")\n",
    "cs.execute(f\"CREATE DATABASE  IF NOT EXISTS {DB}\")\n",
    "cs.execute(f\"CREATE SCHEMA    IF NOT EXISTS {DB}.{SC}\")\n",
    "cs.execute(f\"USE WAREHOUSE {WH}\")\n",
    "cs.execute(f\"USE DATABASE {DB}\")\n",
    "cs.execute(f\"USE SCHEMA {SC}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0xffffabd6fb30>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cs.execute(\"CREATE STAGE IF NOT EXISTS purchases_stage\")\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE FILE FORMAT csv_ff\n",
    "  TYPE = CSV\n",
    "  FIELD_OPTIONALLY_ENCLOSED_BY = '\\\"'\n",
    "  PARSE_HEADER = TRUE\n",
    "  NULL_IF = ('','NULL')\n",
    "  TRIM_SPACE = TRUE\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0xffffabd6fb30>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE purchases_detail (\n",
    "  PurchaseOrderID           STRING,\n",
    "  PurchaseOrderLineID       STRING,\n",
    "  ReceivedOuters            NUMBER(18,4),\n",
    "  ExpectedUnitPricePerOuter NUMBER(18,4),\n",
    "  OrderDate                 DATE,\n",
    "  SupplierID                STRING\n",
    ")\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snowflake 行数: 245490\n",
      "('1', '1', Decimal('18.0000'), Decimal('5.5000'), datetime.date(2019, 1, 1), '2')\n",
      "('1', '2', Decimal('21.0000'), Decimal('5.5000'), datetime.date(2019, 1, 1), '2')\n",
      "('1', '3', Decimal('18.0000'), Decimal('5.5000'), datetime.date(2019, 1, 1), '2')\n",
      "('1', '2', Decimal('21.0000'), Decimal('5.5000'), datetime.date(2019, 1, 1), '2')\n",
      "('1', '1', Decimal('18.0000'), Decimal('5.5000'), datetime.date(2019, 1, 1), '2')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cs.execute(f\"PUT 'file:///{OUT_CSV.as_posix()}' @purchases_stage AUTO_COMPRESS=TRUE OVERWRITE=TRUE\")\n",
    "\n",
    "cs.execute(\"ALTER SESSION SET DATE_INPUT_FORMAT='AUTO'\")\n",
    "cs.execute(\"\"\"\n",
    "COPY INTO purchases_detail\n",
    "FROM @purchases_stage\n",
    "FILE_FORMAT=(FORMAT_NAME='csv_ff')\n",
    "MATCH_BY_COLUMN_NAME=CASE_INSENSITIVE\n",
    "ON_ERROR='ABORT_STATEMENT'\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# 10) 校验\n",
    "cs.execute(\"SELECT COUNT(*) FROM purchases_detail\")\n",
    "print(\"Snowflake 行数:\", cs.fetchone()[0])\n",
    "cs.execute(\"SELECT * FROM purchases_detail ORDER BY OrderDate, PurchaseOrderID LIMIT 5\")\n",
    "for row in cs.fetchall():\n",
    "    print(row)\n",
    "\n",
    "cs.close(); conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Warehouse and Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0xffffabd6c0b0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import snowflake.connector as sf\n",
    "\n",
    "\n",
    "\n",
    "# === 建议用环境变量/ .env ===\n",
    "SNOW_USER=os.getenv(\"SNOW_USER\",\"<mnonog>\")\n",
    "SNOW_PASSWORD=os.getenv(\"SNOW_PASSWORD\",\"<KayaNatinToL0RD!>\")\n",
    "SNOW_ACCOUNT=os.getenv(\"SNOW_ACCOUNT\",\"cejmwpt-djb91267\")   # 只写标识，不要写 https://...\n",
    "SNOW_ROLE=os.getenv(\"SNOW_ROLE\",\"ACCOUNTADMIN\")\n",
    "WH_NAME=os.getenv(\"SNOW_WH\",\"ETL_WH\")\n",
    "DB_NAME=os.getenv(\"SNOW_DB\",\"ETL_DB\")\n",
    "SCHEMA_NAME=os.getenv(\"SNOW_SCHEMA\",\"ETL_SCHEMA\")\n",
    "\n",
    "conn = sf.connect(user='mnonog', password='KayaNatinToL0RD!', account='cejmwpt-djb91267')\n",
    "cs = conn.cursor()\n",
    "\n",
    "# 基础对象\n",
    "cs.execute(f\"CREATE WAREHOUSE IF NOT EXISTS {WH_NAME} WAREHOUSE_SIZE=SMALL AUTO_SUSPEND=60 AUTO_RESUME=TRUE\")\n",
    "cs.execute(f\"CREATE DATABASE  IF NOT EXISTS {DB_NAME}\")\n",
    "cs.execute(f\"CREATE SCHEMA    IF NOT EXISTS {DB_NAME}.{SCHEMA_NAME}\")\n",
    "cs.execute(f\"USE ROLE {SNOW_ROLE}\")\n",
    "cs.execute(f\"USE WAREHOUSE {WH_NAME}\")\n",
    "cs.execute(f\"USE DATABASE {DB_NAME}\")\n",
    "cs.execute(f\"USE SCHEMA {SCHEMA_NAME}\")\n",
    "\n",
    "# 统一 stage / 文件格式\n",
    "cs.execute(\"CREATE STAGE IF NOT EXISTS purchases_stage\")\n",
    "cs.execute(\"CREATE STAGE IF NOT EXISTS invoices_stage\")\n",
    "cs.execute(\"CREATE STAGE IF NOT EXISTS supplier_stage\")\n",
    "\n",
    "cs.execute(\"\"\"CREATE OR REPLACE FILE FORMAT csv_ff \n",
    "  TYPE=CSV FIELD_OPTIONALLY_ENCLOSED_BY='\\\"' SKIP_HEADER=1 NULL_IF=('','NULL')\"\"\")\n",
    "cs.execute(\"\"\"CREATE OR REPLACE FILE FORMAT xml_ff TYPE=XML STRIP_OUTER_ELEMENT=TRUE\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to Snowflakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sf.connect(\n",
    "    user=os.getenv(\"SNOW_USER\", \"mnonog\"),\n",
    "    password=os.getenv(\"SNOW_PASSWORD\", \"KayaNatinToL0RD!\"),\n",
    "    account=os.getenv(\"SNOW_ACCOUNT\", \"cejmwpt-djb91267\")  # 只写标识，不要 https:// 前缀\n",
    ")\n",
    "cs = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Task 1 and 2: Creating Single Table of Purchases Data and Adding the POAmount Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_93494/4052790598.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/4052790598.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/4052790598.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/4052790598.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/4052790598.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/4052790598.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/4052790598.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/4052790598.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/4052790598.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/4052790598.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/4052790598.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/4052790598.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/4052790598.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/4052790598.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/4052790598.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/4052790598.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/4052790598.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/4052790598.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/4052790598.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/4052790598.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/4052790598.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/4052790598.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/4052790598.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/4052790598.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/4052790598.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/4052790598.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/4052790598.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/4052790598.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/4052790598.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/4052790598.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/4052790598.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/4052790598.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/4052790598.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/4052790598.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/4052790598.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/4052790598.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/4052790598.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/4052790598.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/4052790598.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/4052790598.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_93494/4052790598.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combine to 1：/home/jovyan/Desktop/SQL/SQL_FINAL_PROJECT/Data-5/Monthly PO Data/combined_purchases.csv line=253,673\n",
      "Snowflake 行数: 253673\n",
      "('1', '1', Decimal('18.0000'), Decimal('5.5000'), datetime.date(2019, 1, 1), '2')\n",
      "('1', '2', Decimal('21.0000'), Decimal('5.5000'), datetime.date(2019, 1, 1), '2')\n",
      "('1', '3', Decimal('18.0000'), Decimal('5.5000'), datetime.date(2019, 1, 1), '2')\n",
      "('1', '2', Decimal('21.0000'), Decimal('5.5000'), datetime.date(2019, 1, 1), '2')\n",
      "('1', '1', Decimal('18.0000'), Decimal('5.5000'), datetime.date(2019, 1, 1), '2')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0xffffabaee030>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import snowflake.connector as sf\n",
    "\n",
    "\n",
    "WH, DB, SC = \"ETL_WH\", \"ETL_DB\", \"ETL_SCHEMA\"\n",
    "BASE_DIR = Path(r\"/home/jovyan/Desktop/SQL/SQL_FINAL_PROJECT/Data-5/Monthly PO Data\").resolve()\n",
    "OUT_CSV  = BASE_DIR / \"combined_purchases.csv\"   # 统一输出文件\n",
    "\n",
    "# 2) 需要保留并标准化到 Snowflake 的列（目标列名）\n",
    "TARGET_COLS = [\n",
    "    \"PurchaseOrderID\",\n",
    "    \"PurchaseOrderLineID\",\n",
    "    \"ReceivedOuters\",\n",
    "    \"ExpectedUnitPricePerOuter\",\n",
    "    \"OrderDate\",\n",
    "    \"SupplierID\",\n",
    "]\n",
    "\n",
    "# 3) 列名别名表（尽量覆盖你文件里的各种写法；不够的话再加几种）\n",
    "ALIASES = {\n",
    "    \"PurchaseOrderID\":           [\"purchaseorderid\", \"purchase_order_id\", \"poid\", \"orderid\"],\n",
    "    \"PurchaseOrderLineID\":       [\"purchaseorderlineid\", \"purchase_order_line_id\", \"polineid\", \"orderlineid\"],\n",
    "    \"ReceivedOuters\":            [\"receivedouters\", \"received_outers\", \"receivedoutersqty\", \"receivedoutersquantity\", \"received_qty\"],\n",
    "    \"ExpectedUnitPricePerOuter\": [\"expectedunitpriceperouter\", \"expected_unit_price_per_outer\", \"unitpriceperouter\", \"expectedprice\", \"unitprice\"],\n",
    "    \"OrderDate\":                 [\"orderdate\", \"order_date\", \"date\", \"order_dt\"],\n",
    "    \"SupplierID\":                [\"supplierid\", \"supplier_id\", \"vendorid\", \"vendor_id\"],\n",
    "}\n",
    "\n",
    "def norm(name: str) -> str:\n",
    "    return \"\".join(ch.lower() for ch in name if ch.isalnum())\n",
    "\n",
    "def pick_and_rename_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    orig_to_norm = {c: norm(c) for c in df.columns}\n",
    "    norm_to_orig = {v: k for k, v in orig_to_norm.items()}  # 取第一个出现的映射即可\n",
    "\n",
    "    selected = {}\n",
    "    missing  = []\n",
    "    for tgt, alias_list in ALIASES.items():\n",
    "        found = None\n",
    "        for alias in alias_list:\n",
    "            if alias in norm_to_orig:\n",
    "                found = norm_to_orig[alias]\n",
    "                break\n",
    "        if found is None:\n",
    "            missing.append(tgt)\n",
    "        else:\n",
    "            selected[tgt] = found\n",
    "\n",
    "    if missing:\n",
    "        raise ValueError(f\"缺少必须列：{missing}; 文件列={list(df.columns)}\")\n",
    "\n",
    "\n",
    "    out = df[[selected[c] for c in TARGET_COLS]].copy()\n",
    "    out.columns = TARGET_COLS\n",
    "    return out\n",
    "\n",
    "frames = []\n",
    "csv_files = sorted(BASE_DIR.glob(\"*.csv\"))\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"在 {BASE_DIR} 下没有找到 .csv 文件\")\n",
    "\n",
    "for p in csv_files:\n",
    "    df = pd.read_csv(p, dtype=str, keep_default_na=False, na_values=[\"\", \"NULL\"])\n",
    "    df_std = pick_and_rename_columns(df)\n",
    "\n",
    "\n",
    "    for c in [\"ReceivedOuters\", \"ExpectedUnitPricePerOuter\"]:\n",
    "        df_std[c] = pd.to_numeric(df_std[c], errors=\"coerce\")\n",
    "\n",
    "    df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
    "\n",
    "\n",
    "    df_std = df_std.dropna(subset=[\"PurchaseOrderID\", \"PurchaseOrderLineID\", \"OrderDate\"])\n",
    "\n",
    "    df_std[\"ReceivedOuters\"] = df_std[\"ReceivedOuters\"].fillna(0)\n",
    "    df_std[\"ExpectedUnitPricePerOuter\"] = df_std[\"ExpectedUnitPricePerOuter\"].fillna(0)\n",
    "\n",
    "    frames.append(df_std)\n",
    "\n",
    "combined = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "combined.to_csv(OUT_CSV, index=False)\n",
    "print(f\"combine to 1：{OUT_CSV} line={len(combined):,}\")\n",
    "\n",
    "conn = sf.connect(\n",
    "    user=os.getenv(\"SNOW_USER\", \"mnonog\"),\n",
    "    password=os.getenv(\"SNOW_PASSWORD\", \"KayaNatinToL0RD!\"),\n",
    "    account=os.getenv(\"SNOW_ACCOUNT\", \"cejmwpt-djb91267\") \n",
    ")\n",
    "\n",
    "cs = conn.cursor()\n",
    "\n",
    "WH, DB, SC = \"ETL_WH\", \"ETL_DB\", \"ETL_SCHEMA\"\n",
    "cs.execute(f\"CREATE WAREHOUSE IF NOT EXISTS {WH} WAREHOUSE_SIZE=SMALL AUTO_SUSPEND=60 AUTO_RESUME=TRUE\")\n",
    "cs.execute(f\"CREATE DATABASE  IF NOT EXISTS {DB}\")\n",
    "cs.execute(f\"CREATE SCHEMA    IF NOT EXISTS {DB}.{SC}\")\n",
    "cs.execute(f\"USE WAREHOUSE {WH}\")\n",
    "cs.execute(f\"USE DATABASE {DB}\")\n",
    "cs.execute(f\"USE SCHEMA {SC}\")\n",
    "\n",
    "cs.execute(\"CREATE STAGE IF NOT EXISTS purchases_stage\")\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE FILE FORMAT csv_ff\n",
    "  TYPE = CSV\n",
    "  FIELD_OPTIONALLY_ENCLOSED_BY = '\\\"'\n",
    "  PARSE_HEADER = TRUE\n",
    "  NULL_IF = ('','NULL')\n",
    "  TRIM_SPACE = TRUE\n",
    "\"\"\")\n",
    "\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE purchases_detail (\n",
    "  PurchaseOrderID           STRING,\n",
    "  PurchaseOrderLineID       STRING,\n",
    "  ReceivedOuters            NUMBER(18,4),\n",
    "  ExpectedUnitPricePerOuter NUMBER(18,4),\n",
    "  OrderDate                 DATE,\n",
    "  SupplierID                STRING\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "cs.execute(f\"PUT 'file:///{OUT_CSV.as_posix()}' @purchases_stage AUTO_COMPRESS=TRUE OVERWRITE=TRUE\")\n",
    "\n",
    "cs.execute(\"ALTER SESSION SET DATE_INPUT_FORMAT='AUTO'\")\n",
    "cs.execute(\"\"\"\n",
    "COPY INTO purchases_detail\n",
    "FROM @purchases_stage\n",
    "FILE_FORMAT=(FORMAT_NAME='csv_ff')\n",
    "MATCH_BY_COLUMN_NAME=CASE_INSENSITIVE\n",
    "ON_ERROR='ABORT_STATEMENT'\n",
    "\"\"\")\n",
    "\n",
    "# 10) 校验\n",
    "cs.execute(\"SELECT COUNT(*) FROM purchases_detail\")\n",
    "print(\"Snowflake 行数:\", cs.fetchone()[0])\n",
    "cs.execute(\"SELECT * FROM purchases_detail ORDER BY OrderDate, PurchaseOrderID LIMIT 5\")\n",
    "for row in cs.fetchall():\n",
    "    print(row)\n",
    "  \n",
    "    \n",
    "# line-level view\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE VIEW PurchaseOrderTotals AS\n",
    "SELECT\n",
    "  PurchaseOrderID,\n",
    "  PurchaseOrderLineID,\n",
    "  ReceivedOuters,\n",
    "  ExpectedUnitPricePerOuter,\n",
    "  OrderDate,\n",
    "  SupplierID,\n",
    "  SUM(ReceivedOuters * ExpectedUnitPricePerOuter)\n",
    "    OVER (PARTITION BY PurchaseOrderID) AS POAmount\n",
    "FROM purchases_detail\n",
    "ORDER BY PurchaseOrderID, PurchaseOrderLineID\n",
    "\"\"\")\n",
    "\n",
    "# add column (no-op if it already exists; remove OR REPLACE since Snowflake doesn't support it on columns)\n",
    "cs.execute(\"ALTER TABLE purchases_detail ADD COLUMN POAmount NUMBER(18,2)\")\n",
    "\n",
    "# backfill totals (same POAmount for all lines of the same order)\n",
    "cs.execute(\"\"\"\n",
    "UPDATE purchases_detail AS pd\n",
    "SET POAmount = t.POAmount\n",
    "FROM (\n",
    "  SELECT\n",
    "    PurchaseOrderID,\n",
    "    ROUND(SUM(ReceivedOuters * ExpectedUnitPricePerOuter), 2) AS POAmount\n",
    "  FROM purchases_detail\n",
    "  GROUP BY PurchaseOrderID\n",
    ") AS t\n",
    "WHERE pd.PurchaseOrderID = t.PurchaseOrderID\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('invoices_stage/incoming/Supplier Transactions XML.xml.gz', 72528, 'e786d1c65b9bf57cb4f3585a762261c0', 'Thu, 11 Sep 2025 15:31:33 GMT')]\n",
      "supplier_invoices rows: 2438\n",
      "(134, 2, 5, 1, 4, '7290', datetime.date(2019, 1, 2), Decimal('313.50'), Decimal('47.03'), Decimal('360.53'), Decimal('0.00'), datetime.date(2019, 1, 7), True, 4, 'incoming/Supplier Transactions XML.xml.gz')\n",
      "(169, 4, 5, 2, 4, '3898', datetime.date(2019, 1, 2), Decimal('21732.00'), Decimal('3259.80'), Decimal('24991.80'), Decimal('0.00'), datetime.date(2019, 1, 7), True, 4, 'incoming/Supplier Transactions XML.xml.gz')\n",
      "(186, 5, 5, 3, 4, '616', datetime.date(2019, 1, 2), Decimal('2740.50'), Decimal('411.11'), Decimal('3151.61'), Decimal('0.00'), datetime.date(2019, 1, 7), True, 4, 'incoming/Supplier Transactions XML.xml.gz')\n",
      "(215, 7, 5, 4, 4, '3869', datetime.date(2019, 1, 2), Decimal('42481.20'), Decimal('6372.19'), Decimal('48853.39'), Decimal('0.00'), datetime.date(2019, 1, 7), True, 4, 'incoming/Supplier Transactions XML.xml.gz')\n",
      "(224, 10, 5, 5, 4, '4697', datetime.date(2019, 1, 2), Decimal('35067.50'), Decimal('5260.14'), Decimal('40327.64'), Decimal('0.00'), datetime.date(2019, 1, 7), True, 4, 'incoming/Supplier Transactions XML.xml.gz')\n"
     ]
    }
   ],
   "source": [
    "# === 3) Extract & load supplier invoice XML (one row per invoice) ===\n",
    "\n",
    "# File format + stage (idempotent)\n",
    "cs.execute(\"CREATE STAGE IF NOT EXISTS invoices_stage\")\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE FILE FORMAT xml_ff\n",
    "  TYPE = XML\n",
    "  STRIP_OUTER_ELEMENT = TRUE\n",
    "\"\"\")\n",
    "\n",
    "# Your XML file path\n",
    "XML_FILE = Path(\"/home/jovyan/Desktop/SQL/SQL_FINAL_PROJECT/Data-5/Supplier Transactions XML.xml\").resolve()\n",
    "\n",
    "# Keep XML isolated in a subfolder\n",
    "cs.execute(f\"PUT 'file:///{XML_FILE.as_posix()}' @invoices_stage/incoming AUTO_COMPRESS=TRUE OVERWRITE=TRUE\")\n",
    "\n",
    "# (optional) sanity check\n",
    "cs.execute(\"LIST @invoices_stage/incoming\"); print(cs.fetchall())\n",
    "\n",
    "# 1) Land raw XML into a VARIANT table (simple COPY is required)\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE supplier_invoices_raw (\n",
    "  doc VARIANT,\n",
    "  sourcefile STRING\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "cs.execute(\"\"\"\n",
    "COPY INTO supplier_invoices_raw (doc, sourcefile)\n",
    "FROM (\n",
    "  SELECT $1, METADATA$FILENAME\n",
    "  FROM @invoices_stage/incoming (FILE_FORMAT => 'xml_ff')\n",
    ")\n",
    "ON_ERROR='ABORT_STATEMENT'\n",
    "\"\"\")\n",
    "\n",
    "# 2) Final relational table (one row = one invoice)\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE supplier_invoices (\n",
    "  SupplierTransactionID NUMBER,\n",
    "  SupplierID            NUMBER,\n",
    "  TransactionTypeID     NUMBER,\n",
    "  PurchaseOrderID       NUMBER,\n",
    "  PaymentMethodID       NUMBER,\n",
    "  SupplierInvoiceNumber STRING,\n",
    "  TransactionDate       DATE,\n",
    "  AmountExcludingTax    NUMBER(18,2),\n",
    "  TaxAmount             NUMBER(18,2),\n",
    "  TransactionAmount     NUMBER(18,2),\n",
    "  OutstandingBalance    NUMBER(18,2),\n",
    "  FinalizationDate      DATE,\n",
    "  IsFinalized           BOOLEAN,\n",
    "  LastEditedBy          NUMBER,\n",
    "  SourceFile            STRING\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "cs.execute(\"ALTER SESSION SET DATE_INPUT_FORMAT='AUTO', TIMESTAMP_INPUT_FORMAT='AUTO'\")\n",
    "\n",
    "# 3) Shred XML from RAW -> FINAL using the hinted functions\n",
    "#    (handles either <root><row>...</row></root> OR already-row-level documents)\n",
    "cs.execute(\"\"\"\n",
    "INSERT INTO supplier_invoices\n",
    "SELECT\n",
    "  TRY_TO_NUMBER(GET(XMLGET(f.value,'SupplierTransactionID'), '$')::string)              AS SupplierTransactionID,\n",
    "  TRY_TO_NUMBER(GET(XMLGET(f.value,'SupplierID'), '$')::string)                         AS SupplierID,\n",
    "  TRY_TO_NUMBER(GET(XMLGET(f.value,'TransactionTypeID'), '$')::string)                  AS TransactionTypeID,\n",
    "  TRY_TO_NUMBER(GET(XMLGET(f.value,'PurchaseOrderID'), '$')::string)                    AS PurchaseOrderID,\n",
    "  TRY_TO_NUMBER(GET(XMLGET(f.value,'PaymentMethodID'), '$')::string)                    AS PaymentMethodID,\n",
    "  XMLGET(f.value,'SupplierInvoiceNumber'):\"$\"::string                                    AS SupplierInvoiceNumber,\n",
    "  TRY_TO_DATE(XMLGET(f.value,'TransactionDate'):\"$\"::string)                            AS TransactionDate,\n",
    "  TRY_TO_NUMBER(REPLACE(XMLGET(f.value,'AmountExcludingTax'):\"$\"::string, ',', ''),38,2) AS AmountExcludingTax,\n",
    "  TRY_TO_NUMBER(REPLACE(XMLGET(f.value,'TaxAmount'):\"$\"::string, ',', ''),38,2)          AS TaxAmount,\n",
    "  TRY_TO_NUMBER(REPLACE(XMLGET(f.value,'TransactionAmount'):\"$\"::string, ',', ''),38,2)  AS TransactionAmount,\n",
    "  TRY_TO_NUMBER(REPLACE(XMLGET(f.value,'OutstandingBalance'):\"$\"::string, ',', ''),38,2) AS OutstandingBalance,\n",
    "  TRY_TO_DATE(XMLGET(f.value,'FinalizationDate'):\"$\"::string)                           AS FinalizationDate,\n",
    "  IFF(LOWER(NULLIF(XMLGET(f.value,'IsFinalized'):\"$\"::string,'')) IN ('1','true','yes'),\n",
    "      TRUE, FALSE)                                                                       AS IsFinalized,\n",
    "  TRY_TO_NUMBER(XMLGET(f.value,'LastEditedBy'):\"$\"::string)                             AS LastEditedBy,\n",
    "  r.sourcefile                                                                           AS SourceFile\n",
    "FROM supplier_invoices_raw r,\n",
    "     LATERAL FLATTEN(\n",
    "       input => IFF(IS_ARRAY(r.doc:root.row), r.doc:root.row, ARRAY_CONSTRUCT(r.doc))\n",
    "     ) f\n",
    "\"\"\")\n",
    "\n",
    "# verify\n",
    "cs.execute(\"SELECT COUNT(*) FROM supplier_invoices\")\n",
    "print(\"supplier_invoices rows:\", cs.fetchone()[0])\n",
    "cs.execute(\"SELECT * FROM supplier_invoices ORDER BY TransactionDate, SupplierTransactionID LIMIT 5\")\n",
    "for r in cs.fetchall():\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0xffffabaee030>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.execute(f\"\"\"\n",
    "CREATE OR REPLACE VIEW {DB}.{SC}.po_invoice_join_vw AS\n",
    "SELECT\n",
    "  d.PurchaseOrderID,\n",
    "  d.PurchaseOrderLineID,\n",
    "  d.SupplierID,\n",
    "  d.OrderDate,\n",
    "  d.ReceivedOuters,\n",
    "  d.ExpectedUnitPricePerOuter,\n",
    "  i.SupplierTransactionID,\n",
    "  i.SupplierInvoiceNumber,\n",
    "  i.TransactionDate   AS InvoiceDate,\n",
    "  i.TransactionAmount AS InvoiceAmount\n",
    "FROM {DB}.{SC}.purchases_detail d\n",
    "JOIN {DB}.{SC}.supplier_invoices i\n",
    "  ON TRY_TO_NUMBER(d.PurchaseOrderID) = i.PurchaseOrderID\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Materialized view not supported for this definition; creating TABLE instead. Reason: 001998 (42710): SQL compilation error:\n",
      "Object 'PURCHASE_ORDERS_AND_INVOICES' already exists as TABLE\n",
      "Created TABLE: ETL_DB.ETL_SCHEMA.purchase_orders_and_invoices\n",
      "Rows in purchase_orders_and_invoices: 2023\n",
      "(1, 2, Decimal('9718.50'), Decimal('313.50'), Decimal('-9405.00'))\n",
      "(2, 4, Decimal('673692.00'), Decimal('21732.00'), Decimal('-651960.00'))\n",
      "(3, 5, Decimal('84955.50'), Decimal('2740.50'), Decimal('-82215.00'))\n",
      "(4, 7, Decimal('1316917.20'), Decimal('42481.20'), Decimal('-1274436.00'))\n",
      "(5, 10, Decimal('1087092.50'), Decimal('35067.50'), Decimal('-1052025.00'))\n",
      "(6, 12, Decimal('171383.50'), Decimal('5528.50'), Decimal('-165855.00'))\n",
      "(7, 4, Decimal('310015.50'), Decimal('10000.50'), Decimal('-300015.00'))\n",
      "(8, 5, Decimal('20367.00'), Decimal('657.00'), Decimal('-19710.00'))\n",
      "(9, 7, Decimal('287726.50'), Decimal('9281.50'), Decimal('-278445.00'))\n",
      "(10, 10, Decimal('32162.50'), Decimal('1037.50'), Decimal('-31125.00'))\n"
     ]
    }
   ],
   "source": [
    "# === 5) purchase_orders_and_invoices (MV if possible, else TABLE) ===\n",
    "\n",
    "# 5.1 Build a PO header total at the order level (typed as NUMBER to match invoices)\n",
    "cs.execute(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {DB}.{SC}.po_header_totals_tmp AS\n",
    "SELECT\n",
    "  TRY_TO_NUMBER(PurchaseOrderID) AS PurchaseOrderID,\n",
    "  TRY_TO_NUMBER(SupplierID)      AS SupplierID,\n",
    "  MIN(OrderDate)                 AS OrderDate,\n",
    "  ROUND(SUM(ReceivedOuters * ExpectedUnitPricePerOuter), 2) AS POAmount\n",
    "FROM {DB}.{SC}.purchases_detail\n",
    "GROUP BY 1,2\n",
    "\"\"\")\n",
    "\n",
    "# 5.2 Attempt Materialized View (fallback to TABLE if MV not supported for this join)\n",
    "create_mv_sql = f\"\"\"\n",
    "CREATE OR REPLACE MATERIALIZED VIEW {DB}.{SC}.purchase_orders_and_invoices AS\n",
    "SELECT\n",
    "  p.PurchaseOrderID,\n",
    "  p.SupplierID,\n",
    "  p.OrderDate,\n",
    "  p.POAmount,\n",
    "  i.SupplierTransactionID,\n",
    "  i.SupplierInvoiceNumber,\n",
    "  i.TransactionDate      AS InvoiceDate,\n",
    "  i.AmountExcludingTax,\n",
    "  i.TransactionAmount,\n",
    "  ROUND(i.AmountExcludingTax - p.POAmount, 2) AS invoiced_vs_quoted\n",
    "FROM {DB}.{SC}.po_header_totals_tmp p\n",
    "JOIN {DB}.{SC}.supplier_invoices i\n",
    "  ON p.PurchaseOrderID = i.PurchaseOrderID\n",
    " AND p.SupplierID      = i.SupplierID\n",
    "\"\"\"\n",
    "\n",
    "create_tbl_sql = f\"\"\"\n",
    "CREATE OR REPLACE TABLE {DB}.{SC}.purchase_orders_and_invoices AS\n",
    "SELECT\n",
    "  p.PurchaseOrderID,\n",
    "  p.SupplierID,\n",
    "  p.OrderDate,\n",
    "  p.POAmount,\n",
    "  i.SupplierTransactionID,\n",
    "  i.SupplierInvoiceNumber,\n",
    "  i.TransactionDate      AS InvoiceDate,\n",
    "  i.AmountExcludingTax,\n",
    "  i.TransactionAmount,\n",
    "  ROUND(i.AmountExcludingTax - p.POAmount, 2) AS invoiced_vs_quoted\n",
    "FROM {DB}.{SC}.po_header_totals_tmp p\n",
    "JOIN {DB}.{SC}.supplier_invoices i\n",
    "  ON p.PurchaseOrderID = i.PurchaseOrderID\n",
    " AND p.SupplierID      = i.SupplierID\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    cs.execute(create_mv_sql)\n",
    "    print(\"Created MATERIALIZED VIEW:\", f\"{DB}.{SC}.purchase_orders_and_invoices\")\n",
    "except Exception as e:\n",
    "    print(\"Materialized view not supported for this definition; creating TABLE instead. Reason:\", str(e)[:140])\n",
    "    cs.execute(create_tbl_sql)\n",
    "    print(\"Created TABLE:\", f\"{DB}.{SC}.purchase_orders_and_invoices\")\n",
    "\n",
    "# 5.3 Quick sanity checks\n",
    "cs.execute(f\"SELECT COUNT(*) FROM {DB}.{SC}.purchase_orders_and_invoices\")\n",
    "print(\"Rows in purchase_orders_and_invoices:\", cs.fetchone()[0])\n",
    "\n",
    "cs.execute(f\"\"\"\n",
    "SELECT PurchaseOrderID, SupplierID, POAmount, AmountExcludingTax, invoiced_vs_quoted\n",
    "FROM {DB}.{SC}.purchase_orders_and_invoices\n",
    "ORDER BY PurchaseOrderID\n",
    "LIMIT 10\n",
    "\"\"\")\n",
    "for r in cs.fetchall():\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported Postgres supplier_case -> /home/jovyan/Desktop/SQL/SQL_FINAL_PROJECT/Data-5/supplier_case.csv\n",
      "[('supplier_stage/pg_export/supplier_case.csv.gz', 1968, '1dbb820e707311aae59f1b3a8a54181b', 'Thu, 11 Sep 2025 15:31:40 GMT')]\n",
      "DDL:\n",
      " CREATE OR REPLACE TABLE ETL_DB.ETL_SCHEMA.supplier_case (\n",
      "  \"supplierid\" NUMBER(38,0),\n",
      "  \"suppliername\" STRING,\n",
      "  \"suppliercategoryid\" NUMBER(38,0),\n",
      "  \"primarycontactpersonid\" NUMBER(38,0),\n",
      "  \"alternatecontactpersonid\" NUMBER(38,0),\n",
      "  \"deliverymethodid\" NUMBER(38,0),\n",
      "  \"postalcityid\" NUMBER(38,0),\n",
      "  \"supplierreference\" STRING,\n",
      "  \"bankaccountname\" STRING,\n",
      "  \"bankaccountbranch\" STRING,\n",
      "  \"bankaccountcode\" NUMBER(38,0),\n",
      "  \"bankaccountnumber\" NUMBER(38,0),\n",
      "  \"bankinternationalcode\" NUMBER(38,0),\n",
      "  \"paymentdays\" NUMBER(38,0),\n",
      "  \"internalcomments\" STRING,\n",
      "  \"phonenumber\" STRING,\n",
      "  \"faxnumber\" STRING,\n",
      "  \"websiteurl\" STRING,\n",
      "  \"deliveryaddressline1\" STRING,\n",
      "  \"deliveryaddressline2\" STRING,\n",
      "  \"deliverypostalcode\" NUMBER(38,0),\n",
      "  \"deliverylocation\" STRING,\n",
      "  \"postaladdressline1\" STRING,\n",
      "  \"postaladdressline2\" STRING,\n",
      "  \"postalpostalcode\" NUMBER(38,0),\n",
      "  \"lasteditedby\" BOOLEAN,\n",
      "  \"validfrom\" STRING,\n",
      "  \"validto\" STRING\n",
      ")\n",
      "supplier_case rows: 13\n"
     ]
    }
   ],
   "source": [
    "# === 6) Extract supplier_case from Postgres → local CSV → Snowflake stage ===\n",
    "# Requires: pip install psycopg2-binary\n",
    "\n",
    "import psycopg2, re\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- Postgres connection (env or defaults) ----\n",
    "PGHOST = os.getenv(\"PGHOST\", \"127.0.0.1\")\n",
    "PGPORT = int(os.getenv(\"PGPORT\", \"8765\"))\n",
    "PGUSER = os.getenv(\"PGUSER\", \"jovyan\")\n",
    "PGPASSWORD = os.getenv(\"PGPASSWORD\", \"postgres\")\n",
    "PGDATABASE = os.getenv(\"PGDATABASE\", \"WestCoastImporters\")\n",
    "\n",
    "EXPORT_DIR = Path(\"/home/jovyan/Desktop/SQL/SQL_FINAL_PROJECT/Data-5\").resolve()\n",
    "EXPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PG_CSV = EXPORT_DIR / \"supplier_case.csv\"\n",
    "\n",
    "# 6a) Postgres → CSV (server-side COPY to client, no pandas DataFrame)\n",
    "with psycopg2.connect(\n",
    "    host=PGHOST, port=PGPORT, user=PGUSER, password=PGPASSWORD, dbname=PGDATABASE\n",
    ") as pg_conn, pg_conn.cursor() as cur, open(PG_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    # If your table is in a schema, qualify it: schema_name.supplier_case\n",
    "    cur.copy_expert(\"COPY supplier_case TO STDOUT WITH CSV HEADER\", f)\n",
    "\n",
    "print(f\"Exported Postgres supplier_case -> {PG_CSV}\")\n",
    "\n",
    "# 6b) Push CSV into Snowflake stage (keep it tidy in a subfolder)\n",
    "cs.execute(f\"CREATE STAGE IF NOT EXISTS {DB}.{SC}.supplier_stage\")\n",
    "cs.execute(f\"PUT 'file:///{PG_CSV.as_posix()}' @{DB}.{SC}.supplier_stage/pg_export AUTO_COMPRESS=TRUE OVERWRITE=TRUE\")\n",
    "cs.execute(f\"LIST @{DB}.{SC}.supplier_stage/pg_export\")\n",
    "print(cs.fetchall())\n",
    "\n",
    "# ---- Helper: infer Snowflake column types from CSV sample ----\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "_int_re   = re.compile(r\"^[+-]?\\d+$\")\n",
    "_float_re = re.compile(r\"^[+-]?\\d*\\.\\d+$\")\n",
    "_bool_set = {\"true\",\"false\",\"t\",\"f\",\"1\",\"0\",\"yes\",\"no\"}\n",
    "\n",
    "def _looks_bool(v:str)->bool:\n",
    "    return v.lower() in _bool_set\n",
    "\n",
    "def _looks_int(v:str)->bool:\n",
    "    return _int_re.match(v) is not None\n",
    "\n",
    "def _looks_float(v:str)->bool:\n",
    "    return _float_re.match(v) is not None\n",
    "\n",
    "def _looks_date(v:str)->bool:\n",
    "    # let Snowflake parse flexibly; we only do a quick sniff\n",
    "    for fmt in (\"%Y-%m-%d\",\"%m/%d/%Y\",\"%Y/%m/%d\"):\n",
    "        try:\n",
    "            datetime.strptime(v, fmt); return True\n",
    "        except: pass\n",
    "    return False\n",
    "\n",
    "def _looks_ts(v:str)->bool:\n",
    "    for fmt in (\"%Y-%m-%d %H:%M:%S\",\"%Y-%m-%dT%H:%M:%S\",\"%m/%d/%Y %H:%M:%S\"):\n",
    "        try:\n",
    "            datetime.strptime(v, fmt); return True\n",
    "        except: pass\n",
    "    return False\n",
    "\n",
    "def generate_snowflake_ddl(csv_path: Path, fq_table: str, sample_rows:int=2000)->str:\n",
    "    with open(csv_path, newline=\"\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        cols = reader.fieldnames\n",
    "        # flags per column\n",
    "        flags = {c: {\"bool\":True,\"int\":True,\"float\":True,\"date\":True,\"ts\":True,\"scale\":0} for c in cols}\n",
    "        n=0\n",
    "        for row in reader:\n",
    "            n += 1\n",
    "            for c in cols:\n",
    "                v = (row[c] or \"\").strip()\n",
    "                if v==\"\":\n",
    "                    continue\n",
    "                if not _looks_bool(v):  flags[c][\"bool\"]=False\n",
    "                if _looks_int(v):\n",
    "                    pass\n",
    "                else:\n",
    "                    flags[c][\"int\"]=False\n",
    "                if _looks_float(v):\n",
    "                    # track decimal scale\n",
    "                    try:\n",
    "                        sc = len(v.split(\".\")[1])\n",
    "                        flags[c][\"scale\"] = max(flags[c][\"scale\"], sc)\n",
    "                    except: pass\n",
    "                else:\n",
    "                    flags[c][\"float\"]=False\n",
    "                if not _looks_date(v): flags[c][\"date\"]=False\n",
    "                if not _looks_ts(v):   flags[c][\"ts\"]=False\n",
    "            if n>=sample_rows: break\n",
    "\n",
    "    def pick_type(fl):\n",
    "        if fl[\"ts\"]:   return \"TIMESTAMP_NTZ\"\n",
    "        if fl[\"date\"]: return \"DATE\"\n",
    "        if fl[\"bool\"]: return \"BOOLEAN\"\n",
    "        if fl[\"int\"]:  return \"NUMBER(38,0)\"\n",
    "        if fl[\"float\"]:\n",
    "            sc = min(max(fl[\"scale\"], 2), 9)\n",
    "            return f\"NUMBER(38,{sc})\"\n",
    "        return \"STRING\"\n",
    "\n",
    "    # quote identifiers safely (handle spaces/mixed case)\n",
    "    def qident(name:str)->str:\n",
    "        return '\"' + name.replace('\"','\"\"') + '\"'\n",
    "\n",
    "    cols_sql = \",\\n  \".join(f\"{qident(c)} {pick_type(flags[c])}\" for c in cols)\n",
    "    return f\"CREATE OR REPLACE TABLE {fq_table} (\\n  {cols_sql}\\n)\"\n",
    "\n",
    "# 6c) Generate CREATE TABLE from CSV header + sample values\n",
    "fq_tbl = f\"{DB}.{SC}.supplier_case\"\n",
    "ddl = generate_snowflake_ddl(PG_CSV, fq_tbl, sample_rows=5000)\n",
    "print(\"DDL:\\n\", ddl)\n",
    "cs.execute(ddl)\n",
    "\n",
    "# 6d) COPY into the Snowflake table\n",
    "cs.execute(\"ALTER SESSION SET DATE_INPUT_FORMAT='AUTO', TIME_INPUT_FORMAT='AUTO', TIMESTAMP_INPUT_FORMAT='AUTO'\")\n",
    "cs.execute(f\"\"\"\n",
    "COPY INTO {fq_tbl}\n",
    "FROM @{DB}.{SC}.supplier_stage/pg_export\n",
    "FILE_FORMAT=(FORMAT_NAME='{DB}.{SC}.csv_ff')\n",
    "MATCH_BY_COLUMN_NAME=CASE_INSENSITIVE\n",
    "PATTERN='.*supplier_case\\\\.csv\\\\.gz'\n",
    "ON_ERROR='ABORT_STATEMENT'\n",
    "\"\"\")\n",
    "\n",
    "# quick check\n",
    "cs.execute(f\"SELECT COUNT(*) FROM {fq_tbl}\")\n",
    "print(\"supplier_case rows:\", cs.fetchone()[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Task 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created TABLE (MV not allowed): ETL_DB.ETL_SCHEMA.supplier_zip_code_weather\n",
      "Rows in supplier_zip_code_weather: 29595\n",
      "('22202', datetime.date(2000, 1, 1), Decimal('13.300000'))\n",
      "('22202', datetime.date(2000, 1, 2), Decimal('20.000000'))\n",
      "('22202', datetime.date(2000, 1, 3), Decimal('20.000000'))\n",
      "('22202', datetime.date(2000, 1, 4), Decimal('21.700000'))\n",
      "('22202', datetime.date(2000, 1, 5), Decimal('8.900000'))\n",
      "('22202', datetime.date(2000, 1, 6), Decimal('8.300000'))\n",
      "('22202', datetime.date(2000, 1, 7), Decimal('12.200000'))\n",
      "('22202', datetime.date(2000, 1, 8), Decimal('7.200000'))\n",
      "('22202', datetime.date(2000, 1, 9), Decimal('8.300000'))\n",
      "('22202', datetime.date(2000, 1, 10), Decimal('11.100000'))\n"
     ]
    }
   ],
   "source": [
    "# === 7) Build supplier_zip_code_weather using ONLY the two NOAA Marketplace tables ===\n",
    "# Prereqs: {DB}.{SC}.supplier_case exists and contains a postal/zip column.\n",
    "\n",
    "# 7.0 Find a ZIP/Postal column in supplier_case\n",
    "def qident(name: str) -> str:\n",
    "    return '\"' + name.replace('\"', '\"\"') + '\"'\n",
    "\n",
    "cs.execute(f\"\"\"\n",
    "SELECT column_name\n",
    "FROM {DB}.INFORMATION_SCHEMA.COLUMNS\n",
    "WHERE table_schema = '{SC}'\n",
    "  AND table_name = 'SUPPLIER_CASE'\n",
    "  AND (\n",
    "        LOWER(column_name) LIKE '%postal%'\n",
    "     OR LOWER(column_name) LIKE '%zip%'\n",
    "     OR LOWER(column_name) LIKE '%zipcode%'\n",
    "     OR LOWER(column_name) LIKE '%zip_code%'\n",
    "  )\n",
    "ORDER BY ordinal_position\n",
    "\"\"\")\n",
    "zip_candidates = [r[0] for r in cs.fetchall()]\n",
    "if not zip_candidates:\n",
    "    raise RuntimeError(\"No ZIP/Postal column found in supplier_case.\")\n",
    "SUP_ZIP_COL_Q = qident(zip_candidates[0])\n",
    "\n",
    "# 7.1 Unique supplier ZIPs normalized to 5 digits\n",
    "cs.execute(f\"\"\"\n",
    "CREATE OR REPLACE VIEW {DB}.{SC}.supplier_zip_vw AS\n",
    "SELECT DISTINCT\n",
    "  LPAD(SUBSTR(REGEXP_REPLACE(TO_VARCHAR({SUP_ZIP_COL_Q}), '[^0-9]', ''), 1, 5), 5, '0') AS zip\n",
    "FROM {DB}.{SC}.supplier_case\n",
    "WHERE {SUP_ZIP_COL_Q} IS NOT NULL AND TRIM(TO_VARCHAR({SUP_ZIP_COL_Q})) <> ''\n",
    "\"\"\")\n",
    "\n",
    "# 7.2 Station ZIP extracted from NOAA station index (use ZIP_GEO_ID or ZIP_NAME)\n",
    "#     We do NOT use any geography/centroids—only these two NOAA tables.\n",
    "cs.execute(f\"\"\"\n",
    "CREATE OR REPLACE VIEW {DB}.{SC}.station_zip_vw AS\n",
    "SELECT\n",
    "  COALESCE(\n",
    "    REGEXP_SUBSTR(ZIP_GEO_ID, '\\\\\\\\d{{5}}'),\n",
    "    REGEXP_SUBSTR(ZIP_NAME,   '\\\\\\\\d{{5}}')\n",
    "  ) AS zip,\n",
    "  NOAA_WEATHER_STATION_ID\n",
    "FROM WEATHER__ENVIRONMENT.CYBERSYN.NOAA_WEATHER_STATION_INDEX\n",
    "WHERE COALESCE(ZIP_GEO_ID, ZIP_NAME) IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "# 7.3 Candidate stations for each supplier ZIP\n",
    "cs.execute(f\"\"\"\n",
    "CREATE OR REPLACE VIEW {DB}.{SC}.zip_station_candidates_vw AS\n",
    "SELECT sz.zip AS PostalPostalCode, st.NOAA_WEATHER_STATION_ID\n",
    "FROM {DB}.{SC}.supplier_zip_vw sz\n",
    "JOIN {DB}.{SC}.station_zip_vw st\n",
    "  ON st.zip = sz.zip\n",
    "\"\"\")\n",
    "\n",
    "# 7.4 Pick ONE \"best\" station per ZIP:\n",
    "#     Define \"best\" as the station with the most TMAX observations overall (uses only TIMESERIES).\n",
    "cs.execute(f\"\"\"\n",
    "CREATE OR REPLACE VIEW {DB}.{SC}.zip_best_station_vw AS\n",
    "WITH coverage AS (\n",
    "  SELECT\n",
    "    c.PostalPostalCode,\n",
    "    c.NOAA_WEATHER_STATION_ID,\n",
    "    COUNT(*) AS obs_days\n",
    "  FROM {DB}.{SC}.zip_station_candidates_vw c\n",
    "  JOIN WEATHER__ENVIRONMENT.CYBERSYN.NOAA_WEATHER_METRICS_TIMESERIES ts\n",
    "    ON ts.noaa_weather_station_id = c.noaa_weather_station_id\n",
    "   AND (\n",
    "        UPPER(ts.variable) IN ('TMAX','TMAX_C','TMAX_F')\n",
    "     OR LOWER(ts.variable_name) LIKE '%max%temp%'\n",
    "   )\n",
    "  GROUP BY 1,2\n",
    ")\n",
    "SELECT *\n",
    "FROM coverage\n",
    "QUALIFY ROW_NUMBER() OVER (\n",
    "  PARTITION BY PostalPostalCode\n",
    "  ORDER BY obs_days DESC, NOAA_WEATHER_STATION_ID\n",
    ") = 1\n",
    "\"\"\")\n",
    "\n",
    "# 7.5 Build the final output: (zip_code, date, high_temperature)\n",
    "create_mv_sql = f\"\"\"\n",
    "CREATE OR REPLACE MATERIALIZED VIEW {DB}.{SC}.supplier_zip_code_weather AS\n",
    "SELECT\n",
    "  m.PostalPostalCode AS zip_code,\n",
    "  ts.date            AS date,\n",
    "  ts.value           AS high_temperature\n",
    "FROM {DB}.{SC}.zip_best_station_vw m\n",
    "JOIN WEATHER__ENVIRONMENT.CYBERSYN.NOAA_WEATHER_METRICS_TIMESERIES ts\n",
    "  ON ts.noaa_weather_station_id = m.noaa_weather_station_id\n",
    "WHERE\n",
    "    UPPER(ts.variable) IN ('TMAX','TMAX_C','TMAX_F')\n",
    " OR LOWER(ts.variable_name) LIKE '%max%temp%'\n",
    "\"\"\"\n",
    "\n",
    "# Fallback to TABLE if MV not allowed\n",
    "try:\n",
    "    cs.execute(create_mv_sql)\n",
    "    print(\"Created MATERIALIZED VIEW:\", f\"{DB}.{SC}.supplier_zip_code_weather\")\n",
    "except Exception as e:\n",
    "    cs.execute(create_mv_sql.replace(\"MATERIALIZED VIEW\", \"TABLE\"))\n",
    "    print(\"Created TABLE (MV not allowed):\", f\"{DB}.{SC}.supplier_zip_code_weather\")\n",
    "\n",
    "# Sanity check\n",
    "cs.execute(f\"SELECT COUNT(*) FROM {DB}.{SC}.supplier_zip_code_weather\")\n",
    "print(\"Rows in supplier_zip_code_weather:\", cs.fetchone()[0])\n",
    "\n",
    "cs.execute(f\"\"\"\n",
    "SELECT * FROM {DB}.{SC}.supplier_zip_code_weather\n",
    "ORDER BY zip_code, date\n",
    "LIMIT 10\n",
    "\"\"\")\n",
    "for r in cs.fetchall():\n",
    "    print(r)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
