{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: snowflake-connector-python in /opt/conda/lib/python3.12/site-packages (3.17.3)\n",
      "Requirement already satisfied: asn1crypto<2.0.0,>0.24.0 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (1.5.1)\n",
      "Requirement already satisfied: boto3>=1.24 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (1.40.25)\n",
      "Requirement already satisfied: botocore>=1.24 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (1.40.25)\n",
      "Requirement already satisfied: cffi<2.0.0,>=1.9 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (1.17.1)\n",
      "Requirement already satisfied: cryptography>=3.1.0 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (45.0.4)\n",
      "Requirement already satisfied: pyOpenSSL<26.0.0,>=22.0.0 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (25.1.0)\n",
      "Requirement already satisfied: pyjwt<3.0.0 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (2.10.1)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (2025.2)\n",
      "Requirement already satisfied: requests<3.0.0 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (2.32.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (25.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (2025.6.15)\n",
      "Requirement already satisfied: typing_extensions<5,>=4.3 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (4.14.0)\n",
      "Requirement already satisfied: filelock<4,>=3.5 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (3.19.1)\n",
      "Requirement already satisfied: sortedcontainers>=2.4.0 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (2.4.0)\n",
      "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (4.3.8)\n",
      "Requirement already satisfied: tomlkit in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (0.13.3)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.12/site-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python) (2.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3.0.0->snowflake-connector-python) (2.4.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from boto3>=1.24->snowflake-connector-python) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /opt/conda/lib/python3.12/site-packages (from boto3>=1.24->snowflake-connector-python) (0.13.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.12/site-packages (from botocore>=1.24->snowflake-connector-python) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.24->snowflake-connector-python) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade snowflake-connector-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2 tast 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snowflake line: 204575\n",
      "('1', '1', Decimal('18.0000'), Decimal('5.5000'), datetime.date(2019, 1, 1), '2')\n",
      "('1', '2', Decimal('21.0000'), Decimal('5.5000'), datetime.date(2019, 1, 1), '2')\n",
      "('1', '3', Decimal('18.0000'), Decimal('5.5000'), datetime.date(2019, 1, 1), '2')\n",
      "('1', '2', Decimal('21.0000'), Decimal('5.5000'), datetime.date(2019, 1, 1), '2')\n",
      "('1', '1', Decimal('18.0000'), Decimal('5.5000'), datetime.date(2019, 1, 1), '2')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "import snowflake.connector as sf\n",
    "\n",
    "BASE_DIR = Path(r\"/home/jovyan/MGTA SQL/final project/SQL_FINAL_PROJECT/Data-5/Monthly PO Data\").resolve()   # file base on your local path\n",
    "OUT_CSV  = BASE_DIR / \"combined_purchases.csv\"\n",
    "\n",
    "# ---- 1) Combine csv file\n",
    "if not OUT_CSV.exists():\n",
    "    TARGET_COLS = [\"PurchaseOrderID\",\"PurchaseOrderLineID\",\"ReceivedOuters\",\n",
    "                   \"ExpectedUnitPricePerOuter\",\"OrderDate\",\"SupplierID\"]\n",
    "    ALIASES = {\n",
    "        \"PurchaseOrderID\":           [\"purchaseorderid\",\"purchase_order_id\",\"poid\",\"orderid\"],\n",
    "        \"PurchaseOrderLineID\":       [\"purchaseorderlineid\",\"purchase_order_line_id\",\"polineid\",\"orderlineid\"],\n",
    "        \"ReceivedOuters\":            [\"receivedouters\",\"received_outers\",\"receivedoutersqty\",\"receivedoutersquantity\",\"received_qty\"],\n",
    "        \"ExpectedUnitPricePerOuter\": [\"expectedunitpriceperouter\",\"expected_unit_price_per_outer\",\"unitpriceperouter\",\"expectedprice\",\"unitprice\"],\n",
    "        \"OrderDate\":                 [\"orderdate\",\"order_date\",\"date\",\"order_dt\"],\n",
    "        \"SupplierID\":                [\"supplierid\",\"supplier_id\",\"vendorid\",\"vendor_id\"],\n",
    "    }\n",
    "    def norm(s): return \"\".join(ch.lower() for ch in s if ch.isalnum())\n",
    "    def pick_and_rename(df):\n",
    "        m = {c: norm(c) for c in df.columns}\n",
    "        rev = {}\n",
    "        for k,v in m.items():\n",
    "            if v not in rev: rev[v]=k\n",
    "        sel = {}\n",
    "        miss=[]\n",
    "        for tgt, aliases in ALIASES.items():\n",
    "            found=None\n",
    "            for a in aliases:\n",
    "                if a in rev: found=rev[a]; break\n",
    "            if not found: miss.append(tgt)\n",
    "            else: sel[tgt]=found\n",
    "        if miss: raise ValueError(f\"缺少必须列: {miss}; 文件列={list(df.columns)}\")\n",
    "        out = df[[sel[c] for c in TARGET_COLS]].copy()\n",
    "        out.columns = TARGET_COLS\n",
    "       \n",
    "        for c in [\"ReceivedOuters\",\"ExpectedUnitPricePerOuter\"]:\n",
    "            out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "      \n",
    "        out[\"OrderDate\"] = pd.to_datetime(out[\"OrderDate\"], errors=\"coerce\", format=\"%m/%d/%Y\")\n",
    "        out = out.dropna(subset=[\"PurchaseOrderID\",\"PurchaseOrderLineID\",\"OrderDate\"])\n",
    "        out[\"ReceivedOuters\"] = out[\"ReceivedOuters\"].fillna(0)\n",
    "        out[\"ExpectedUnitPricePerOuter\"] = out[\"ExpectedUnitPricePerOuter\"].fillna(0)\n",
    "        return out\n",
    "\n",
    "    frames=[]\n",
    "    files = sorted(BASE_DIR.glob(\"*.csv\"))\n",
    "    if not files: raise FileNotFoundError(f\"{BASE_DIR} 下没有 .csv\")\n",
    "    for p in files:\n",
    "        df = pd.read_csv(p, dtype=str, keep_default_na=False, na_values=[\"\",\"NULL\"])\n",
    "        frames.append(pick_and_rename(df))\n",
    "    combined = pd.concat(frames, ignore_index=True)\n",
    "    combined.to_csv(OUT_CSV, index=False)\n",
    "    print(f\"已生成本地整合：{OUT_CSV}  行数={len(combined):,}\")\n",
    "\n",
    "# ---- 2) To Snowflake ----\n",
    "conn = sf.connect(\n",
    "    user=os.getenv(\"SNOW_USER\", \"ETHANAN2000\"),\n",
    "    password=os.getenv(\"SNOW_PASSWORD\", \"An67087833@123\"),\n",
    "    account=os.getenv(\"SNOW_ACCOUNT\", \"svogymj-bxb71103\") \n",
    ")\n",
    "cs = conn.cursor()\n",
    "\n",
    "## change these line under to choose where we save the data to\n",
    "WH, DB, SC = \"ETL_WH\",\"ETL_DB\",\"ETL_SCHEMA\"\n",
    "cs.execute(f\"CREATE WAREHOUSE IF NOT EXISTS {WH} WAREHOUSE_SIZE=SMALL AUTO_SUSPEND=60 AUTO_RESUME=TRUE\")\n",
    "cs.execute(f\"CREATE DATABASE  IF NOT EXISTS {DB}\")\n",
    "cs.execute(f\"CREATE SCHEMA    IF NOT EXISTS {DB}.{SC}\")\n",
    "cs.execute(f\"USE WAREHOUSE {WH}\"); cs.execute(f\"USE DATABASE {DB}\"); cs.execute(f\"USE SCHEMA {SC}\")\n",
    "\n",
    "cs.execute(\"CREATE STAGE IF NOT EXISTS purchases_stage\")\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE FILE FORMAT csv_ff\n",
    "  TYPE=CSV\n",
    "  FIELD_OPTIONALLY_ENCLOSED_BY='\\\"'\n",
    "  PARSE_HEADER=TRUE\n",
    "  NULL_IF=('','NULL')\n",
    "  TRIM_SPACE=TRUE\n",
    "\"\"\")\n",
    "\n",
    "# Combine file\n",
    "cs.execute(f\"PUT 'file:///{OUT_CSV.as_posix()}' @purchases_stage AUTO_COMPRESS=TRUE OVERWRITE=TRUE\")\n",
    "\n",
    "staged_name = OUT_CSV.name + \".gz\" \n",
    "copy_sql = f\"\"\"\n",
    "COPY INTO purchases_detail\n",
    "FROM @purchases_stage/{staged_name}\n",
    "FILE_FORMAT=(FORMAT_NAME='csv_ff')\n",
    "MATCH_BY_COLUMN_NAME=CASE_INSENSITIVE\n",
    "ON_ERROR='ABORT_STATEMENT'\n",
    "\"\"\"\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE purchases_detail (\n",
    "  PurchaseOrderID           STRING,\n",
    "  PurchaseOrderLineID       STRING,\n",
    "  ReceivedOuters            NUMBER(18,4),\n",
    "  ExpectedUnitPricePerOuter NUMBER(18,4),\n",
    "  OrderDate                 DATE,\n",
    "  SupplierID                STRING\n",
    ")\n",
    "\"\"\")\n",
    "cs.execute(\"ALTER SESSION SET DATE_INPUT_FORMAT='AUTO'\")\n",
    "cs.execute(copy_sql)\n",
    "\n",
    "cs.execute(\"SELECT COUNT(*) FROM purchases_detail\")\n",
    "print(\"Snowflake line:\", cs.fetchone()[0])\n",
    "cs.execute(\"SELECT * FROM purchases_detail ORDER BY OrderDate, PurchaseOrderID LIMIT 5\")\n",
    "for r in cs.fetchall(): print(r)\n",
    "\n",
    "cs.close(); conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "part 2 tast 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "purchase_order_totals rows: 2025\n"
     ]
    }
   ],
   "source": [
    "# —— 牢靠版：重连 + 上下文一次性设置 + 临时游标 —— \n",
    "import os\n",
    "import snowflake.connector as sf\n",
    "\n",
    "ROLE = os.getenv(\"SNOW_ROLE\", \"ACCOUNTADMIN\")\n",
    "WH   = os.getenv(\"SNOW_WH\",   \"ETL_WH\")\n",
    "DB   = os.getenv(\"SNOW_DB\",   \"ETL_DB\")\n",
    "SC   = os.getenv(\"SNOW_SCHEMA\",\"ETL_SCHEMA\")\n",
    "\n",
    "# 1) 强制建立一个新的连接：把上下文直接放在 connect 里；开启 keep_alive\n",
    "conn = sf.connect(\n",
    "    user=os.getenv(\"SNOW_USER\", \"ETHANAN2000\"),\n",
    "    password=os.getenv(\"SNOW_PASSWORD\", \"An67087833@123\"),\n",
    "    account=os.getenv(\"SNOW_ACCOUNT\", \"svogymj-bxb71103\"), \n",
    "    role=ROLE,\n",
    "    warehouse=WH,\n",
    "    database=DB,\n",
    "    schema=SC,\n",
    "    client_session_keep_alive=True,\n",
    ")\n",
    "\n",
    "# 2) 在同一个 with 里完成所有 SQL；退出 with 后游标会自动关闭\n",
    "with conn.cursor() as cs:\n",
    "    # 行金额视图\n",
    "    cs.execute(\"\"\"\n",
    "    CREATE OR REPLACE VIEW purchases_line AS\n",
    "    SELECT\n",
    "      PurchaseOrderID,\n",
    "      PurchaseOrderLineID,\n",
    "      OrderDate,\n",
    "      SupplierID,\n",
    "      (ReceivedOuters * ExpectedUnitPricePerOuter) AS line_amount\n",
    "    FROM purchases_detail\n",
    "    \"\"\")\n",
    "\n",
    "    # 订单级金额表\n",
    "    cs.execute(\"\"\"\n",
    "    CREATE OR REPLACE TABLE purchase_order_totals AS\n",
    "    SELECT\n",
    "      PurchaseOrderID,\n",
    "      MIN(OrderDate) AS OrderDate,\n",
    "      ANY_VALUE(SupplierID) AS SupplierID,\n",
    "      SUM(line_amount) AS POAmount\n",
    "    FROM purchases_line\n",
    "    GROUP BY PurchaseOrderID\n",
    "    \"\"\")\n",
    "\n",
    "    # 校验\n",
    "    cs.execute(\"SELECT COUNT(*) FROM purchase_order_totals\")\n",
    "    print(\"purchase_order_totals rows:\", cs.fetchone()[0])\n",
    "\n",
    "# 可选：保持连接给后续步骤用；如果不需要就 conn.close()\n",
    "# conn.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "part 2 tast 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "将上传的 XML 文件： /home/jovyan/MGTA SQL/final project/SQL_FINAL_PROJECT/Data-5/Supplier Transactions XML.xml\n",
      "Stage中文件（前几项）： ['invoices_stage/Supplier Transactions XML.xml.gz']\n",
      "invoices_raw rows: 2438\n",
      "invoices rows: 2438\n",
      "invoices 样例：\n",
      "('5', '3', datetime.date(1970, 1, 1), Decimal('7.0000'), '1')\n",
      "('5', '3', datetime.date(1970, 1, 1), Decimal('7.0000'), '1')\n",
      "('5', '3', datetime.date(1970, 1, 1), Decimal('7.0000'), '1')\n",
      "('5', '3', datetime.date(1970, 1, 1), Decimal('7.0000'), '1')\n",
      "('5', '3', datetime.date(1970, 1, 1), Decimal('7.0000'), '1')\n"
     ]
    }
   ],
   "source": [
    "# ==== Task 3：发票 XML 全流程（PUT → 原始装载 → 解析成表）====\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import snowflake.connector as sf\n",
    "\n",
    "# 0) 连接（如果你上面已有 conn，就会复用）\n",
    "try:\n",
    "    conn\n",
    "except NameError:\n",
    "    conn = sf.connect(\n",
    "        user=os.getenv(\"SNOW_USER\",\"<your_user>\"),\n",
    "        password=os.getenv(\"SNOW_PASSWORD\",\"<your_password>\"),\n",
    "        account=os.getenv(\"SNOW_ACCOUNT\",\"svogymj-bxb71103\"),\n",
    "        role=os.getenv(\"SNOW_ROLE\",\"ACCOUNTADMIN\"),\n",
    "        warehouse=os.getenv(\"SNOW_WH\",\"ETL_WH\"),\n",
    "        database=os.getenv(\"SNOW_DB\",\"ETL_DB\"),\n",
    "        schema=os.getenv(\"SNOW_SCHEMA\",\"ETL_SCHEMA\"),\n",
    "        client_session_keep_alive=True,\n",
    "    )\n",
    "\n",
    "# 1) 定位 XML 文件（优先你的工作区路径，找不到则回退 /mnt/data）\n",
    "candidates = [\n",
    "    Path(\"/home/jovyan/MGTA SQL/final project/SQL_FINAL_PROJECT/Data-5/Supplier Transactions XML.xml\"),\n",
    "    Path(\"/mnt/data/Supplier Transactions XML.xml\"),\n",
    "]\n",
    "xml_path = next((p for p in candidates if p.exists()), None)\n",
    "if not xml_path:\n",
    "    raise FileNotFoundError(\"找不到 XML 文件。请确认路径是否正确，或把文件上传到 /mnt/data 后重试。\")\n",
    "print(\"将上传的 XML 文件：\", xml_path.as_posix())\n",
    "\n",
    "with conn.cursor() as cs:\n",
    "    # 2) Stage & 文件格式（幂等）\n",
    "    cs.execute(\"CREATE STAGE IF NOT EXISTS invoices_stage\")\n",
    "    cs.execute(\"CREATE OR REPLACE FILE FORMAT xml_ff TYPE=XML STRIP_OUTER_ELEMENT=TRUE\")\n",
    "\n",
    "    # 3) PUT（覆盖上传）\n",
    "    cs.execute(f\"PUT 'file:///{xml_path.as_posix()}' @invoices_stage AUTO_COMPRESS=TRUE OVERWRITE=TRUE\")\n",
    "    # 简短确认\n",
    "    cs.execute(\"LIST @invoices_stage\")\n",
    "    print(\"Stage中文件（前几项）：\", [r[0] for r in cs.fetchall()[:3]])\n",
    "\n",
    "    # 4) 原始装载到 VARIANT 表（幂等）\n",
    "    cs.execute(\"CREATE OR REPLACE TABLE invoices_raw (v VARIANT)\")\n",
    "    cs.execute(\"\"\"\n",
    "    COPY INTO invoices_raw\n",
    "    FROM @invoices_stage\n",
    "    FILE_FORMAT=(FORMAT_NAME='xml_ff')\n",
    "    ON_ERROR='ABORT_STATEMENT'\n",
    "    \"\"\")\n",
    "    cs.execute(\"SELECT COUNT(*) FROM invoices_raw\")\n",
    "    print(\"invoices_raw rows:\", cs.fetchone()[0])\n",
    "\n",
    "    # 5) 解析为结构化 invoices 表\n",
    "    # - 兼容两种XML形态：单个对象 或 数组/重复元素\n",
    "    # - 使用 COALESCE + 多种日期格式兜底\n",
    "    cs.execute(\"\"\"\n",
    "    CREATE OR REPLACE TABLE invoices AS\n",
    "    WITH flat AS (\n",
    "      SELECT\n",
    "        r.value                           AS obj\n",
    "      FROM invoices_raw,\n",
    "           LATERAL FLATTEN(\n",
    "             input => CASE WHEN TYPEOF(v)='ARRAY' THEN v ELSE ARRAY_CONSTRUCT(v) END\n",
    "           ) AS r\n",
    "    )\n",
    "    SELECT\n",
    "      obj:SupplierInvoiceNumber::string                 AS InvoiceNumber,\n",
    "      obj:PurchaseOrderID::string                       AS PurchaseOrderID,\n",
    "      COALESCE(\n",
    "        TRY_TO_DATE(obj:TransactionDate::string,'YYYY-MM-DD'),\n",
    "        TRY_TO_DATE(obj:TransactionDate::string,'MM/DD/YYYY'),\n",
    "        TRY_TO_DATE(obj:TransactionDate::string,'M/D/YYYY'),\n",
    "        TRY_TO_DATE(obj:TransactionDate::string)\n",
    "      )                                                 AS InvoiceDate,\n",
    "      obj:AmountExcludingTax::number(18,4)              AS AmountExcludingTax,\n",
    "      obj:SupplierID::string                            AS SupplierID\n",
    "    FROM flat\n",
    "    WHERE obj:PurchaseOrderID IS NOT NULL\n",
    "    \"\"\")\n",
    "    # 6) 校验\n",
    "    cs.execute(\"SELECT COUNT(*) FROM invoices\")\n",
    "    print(\"invoices rows:\", cs.fetchone()[0])\n",
    "\n",
    "    cs.execute(\"\"\"\n",
    "      SELECT * FROM invoices\n",
    "      ORDER BY InvoiceDate, PurchaseOrderID\n",
    "      LIMIT 5\n",
    "    \"\"\")\n",
    "    sample = cs.fetchall()\n",
    "    print(\"invoices 样例：\")\n",
    "    for r in sample:\n",
    "        print(r)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "part 2 tast 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "purchase_orders_and_invoices (TABLE) rows: 2438\n",
      "('3', datetime.date(2019, 1, 1), '5', Decimal('76734.00000000'), '5', Decimal('7.0000'), datetime.date(1970, 1, 1), Decimal('-76727.00000000'))\n",
      "('3', datetime.date(2019, 1, 1), '5', Decimal('76734.00000000'), '5', Decimal('7.0000'), datetime.date(1970, 1, 1), Decimal('-76727.00000000'))\n",
      "('3', datetime.date(2019, 1, 1), '5', Decimal('76734.00000000'), '5', Decimal('7.0000'), datetime.date(1970, 1, 1), Decimal('-76727.00000000'))\n",
      "('3', datetime.date(2019, 1, 1), '5', Decimal('76734.00000000'), '5', Decimal('7.0000'), datetime.date(1970, 1, 1), Decimal('-76727.00000000'))\n",
      "('3', datetime.date(2019, 1, 1), '5', Decimal('76734.00000000'), '5', Decimal('7.0000'), datetime.date(1970, 1, 1), Decimal('-76727.00000000'))\n"
     ]
    }
   ],
   "source": [
    "# ==== Task 4：采购 × 发票合并，产出差额 ====\n",
    "import os\n",
    "import snowflake.connector as sf\n",
    "\n",
    "# 复用现有连接；如果不存在就连一次\n",
    "try:\n",
    "    conn\n",
    "except NameError:\n",
    "    conn = sf.connect(\n",
    "        user=os.getenv(\"SNOW_USER\",\"<your_user>\"),\n",
    "        password=os.getenv(\"SNOW_PASSWORD\",\"<your_password>\"),\n",
    "        account=os.getenv(\"SNOW_ACCOUNT\",\"svogymj-bxb71103\"),\n",
    "        role=os.getenv(\"SNOW_ROLE\",\"ACCOUNTADMIN\"),\n",
    "        warehouse=os.getenv(\"SNOW_WH\",\"ETL_WH\"),\n",
    "        database=os.getenv(\"SNOW_DB\",\"ETL_DB\"),\n",
    "        schema=os.getenv(\"SNOW_SCHEMA\",\"ETL_SCHEMA\"),\n",
    "        client_session_keep_alive=True,\n",
    "    )\n",
    "\n",
    "DB = os.getenv(\"SNOW_DB\",\"ETL_DB\")\n",
    "SC = os.getenv(\"SNOW_SCHEMA\",\"ETL_SCHEMA\")\n",
    "\n",
    "core_sql = f\"\"\"\n",
    "SELECT\n",
    "  p.PurchaseOrderID,\n",
    "  p.OrderDate,                 -- 交易日期（若想用发票日期，改成 i.InvoiceDate）\n",
    "  p.SupplierID,\n",
    "  p.POAmount,\n",
    "  i.InvoiceNumber,\n",
    "  i.AmountExcludingTax,\n",
    "  i.InvoiceDate,\n",
    "  (i.AmountExcludingTax - p.POAmount) AS invoiced_vs_quoted\n",
    "FROM {DB}.{SC}.purchase_order_totals p\n",
    "JOIN {DB}.{SC}.invoices i\n",
    "  ON i.PurchaseOrderID = p.PurchaseOrderID\n",
    "WHERE i.InvoiceDate IS NOT NULL       -- 若想保留 1970-01-01 或空日期，请删除该行\n",
    "\"\"\"\n",
    "\n",
    "with conn.cursor() as cs:\n",
    "    # 先清理，幂等\n",
    "    cs.execute(f\"DROP MATERIALIZED VIEW IF EXISTS {DB}.{SC}.purchase_orders_and_invoices\")\n",
    "    cs.execute(f\"DROP TABLE IF EXISTS {DB}.{SC}.purchase_orders_and_invoices\")\n",
    "\n",
    "    # 优先建物化视图；不支持就退回表\n",
    "    try:\n",
    "        cs.execute(f\"CREATE MATERIALIZED VIEW {DB}.{SC}.purchase_orders_and_invoices AS {core_sql}\")\n",
    "        obj = \"MVIEW\"\n",
    "    except Exception as e:\n",
    "        # 某些权限/版本限制下 MVIEW 不可用\n",
    "        cs.execute(f\"CREATE OR REPLACE TABLE {DB}.{SC}.purchase_orders_and_invoices AS {core_sql}\")\n",
    "        obj = \"TABLE\"\n",
    "\n",
    "    # 校验\n",
    "    cs.execute(f\"SELECT COUNT(*) FROM {DB}.{SC}.purchase_orders_and_invoices\")\n",
    "    total = cs.fetchone()[0]\n",
    "    print(f\"purchase_orders_and_invoices ({obj}) rows:\", total)\n",
    "\n",
    "    cs.execute(f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {DB}.{SC}.purchase_orders_and_invoices\n",
    "        ORDER BY OrderDate, PurchaseOrderID\n",
    "        LIMIT 5\n",
    "    \"\"\")\n",
    "    for r in cs.fetchall():\n",
    "        print(r)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "part 2 tast 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "PG = dict(\n",
    "    host=\"localhost\",\n",
    "    port=55432,           # 一定要写 55432\n",
    "    dbname=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"pgpwd123\"   # 就是你 docker run 里设置的 POSTGRES_PASSWORD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ connected: PostgreSQL 15.14 (Debian 15.14-1.pgdg13+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 14.2.0-19) 14.2.0, 64-bit\n"
     ]
    }
   ],
   "source": [
    "import psycopg2, time\n",
    "\n",
    "PG = dict(\n",
    "    host=\"host.docker.internal\",  # 👈 关键：改这里\n",
    "    port=55432,                   # 你映射的端口\n",
    "    dbname=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"pgpwd123\",\n",
    ")\n",
    "\n",
    "deadline = time.time() + 15\n",
    "err = None\n",
    "while time.time() < deadline:\n",
    "    try:\n",
    "        with psycopg2.connect(**PG) as conn, conn.cursor() as cur:\n",
    "            cur.execute(\"select version()\")\n",
    "            print(\"✅ connected:\", cur.fetchone()[0])\n",
    "            err = None\n",
    "            break\n",
    "    except Exception as e:\n",
    "        err = e\n",
    "        time.sleep(1)\n",
    "\n",
    "if err:\n",
    "    raise err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['supplierid', 'suppliername', 'suppliercategoryid', 'primarycontactpersonid', 'alternatecontactpersonid', 'deliverymethodid', 'postalcityid', 'supplierreference', 'bankaccountname', 'bankaccountbranch', 'bankaccountcode', 'bankaccountnumber', 'bankinternationalcode', 'paymentdays', 'internalcomments', 'phonenumber', 'faxnumber', 'websiteurl', 'deliveryaddressline1', 'deliveryaddressline2', 'deliverypostalcode', 'deliverylocation', 'postaladdressline1', 'postaladdressline2', 'postalpostalcode', 'lasteditedby', 'validfrom', 'validto']\n"
     ]
    }
   ],
   "source": [
    "with sf_conn.cursor() as cs:\n",
    "    cs.execute(f\"DESC TABLE {DB}.{SC}.supplier_case\")\n",
    "    print([row[0] for row in cs.fetchall()])  # 第一列是列名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "将执行脚本： /home/jovyan/MGTA SQL/final project/SQL_FINAL_PROJECT/Data-5/supplier_case.pgsql\n",
      "✅ Postgres 就绪\n",
      "✅ 已在 Postgres 执行 supplier_case.pgsql\n",
      "✅ 已导出 CSV： /home/jovyan/MGTA SQL/final project/SQL_FINAL_PROJECT/Data-5/supplier_case.csv\n",
      "CSV 行数： 13\n",
      "CSV 列： ['supplierid', 'suppliername', 'suppliercategoryid', 'primarycontactpersonid', 'alternatecontactpersonid', 'deliverymethodid', 'postalcityid', 'supplierreference', 'bankaccountname', 'bankaccountbranch', 'bankaccountcode', 'bankaccountnumber', 'bankinternationalcode', 'paymentdays', 'internalcomments', 'phonenumber', 'faxnumber', 'websiteurl', 'deliveryaddressline1', 'deliveryaddressline2', 'deliverypostalcode', 'deliverylocation', 'postaladdressline1', 'postaladdressline2', 'postalpostalcode', 'lasteditedby', 'validfrom', 'validto']\n",
      "✅ Snowflake version: 9.27.0\n",
      "识别列映射：SupplierID -> supplierid ，PostalPostalCode -> postalcityid\n",
      "✅ Snowflake.supplier_case 行数： 13\n",
      "✅ supplier_basic 行数： 13\n",
      "样例：\n",
      "(1, 22202)\n",
      "(2, 80125)\n",
      "(3, 60523)\n",
      "(4, 95642)\n",
      "(5, 80125)\n"
     ]
    }
   ],
   "source": [
    "# ==== Task 5 最终版：PG → CSV → Snowflake → supplier_basic ====\n",
    "import os, time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import snowflake.connector as sf\n",
    "\n",
    "# -----------------------------\n",
    "# 1) 连接 Docker 里的 Postgres\n",
    "# -----------------------------\n",
    "PG = dict(\n",
    "    host=\"host.docker.internal\",\n",
    "    port=55432,\n",
    "    dbname=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"pgpwd123\",\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 2) 找到 .pgsql 脚本\n",
    "# -----------------------------\n",
    "pgsql_candidates = [\n",
    "    Path(\"/home/jovyan/MGTA SQL/final project/SQL_FINAL_PROJECT/Data-5/supplier_case.pgsql\"),\n",
    "    Path(\"/mnt/data/supplier_case.pgsql\"),\n",
    "]\n",
    "pgsql_file = next((p for p in pgsql_candidates if p.exists()), None)\n",
    "if not pgsql_file:\n",
    "    raise FileNotFoundError(\"找不到 supplier_case.pgsql，请确认路径。\")\n",
    "print(\"将执行脚本：\", pgsql_file.as_posix())\n",
    "\n",
    "# -----------------------------\n",
    "# 3) 等待 PG 就绪（最多 30 秒）\n",
    "# -----------------------------\n",
    "deadline = time.time() + 30\n",
    "while time.time() < deadline:\n",
    "    try:\n",
    "        with psycopg2.connect(**PG) as _:\n",
    "            break\n",
    "    except Exception:\n",
    "        time.sleep(1)\n",
    "else:\n",
    "    raise RuntimeError(\"等待 Postgres 超时，请确认容器已启动并监听 55432 端口。\")\n",
    "print(\"✅ Postgres 就绪\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 4) 在 PG 执行 .pgsql（创建并填充 supplier_case）\n",
    "# ---------------------------------------------------\n",
    "with psycopg2.connect(**PG) as pgconn, pgconn.cursor() as cur, open(pgsql_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    cur.execute(f.read())\n",
    "    pgconn.commit()\n",
    "print(\"✅ 已在 Postgres 执行 supplier_case.pgsql\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 5) 从 PG 导出 CSV 到项目目录（自动 mkdir）\n",
    "# ---------------------------------------------------\n",
    "proj_dir = Path(\"/home/jovyan/MGTA SQL/final project/SQL_FINAL_PROJECT/Data-5\")\n",
    "proj_dir.mkdir(parents=True, exist_ok=True)\n",
    "csv_out = proj_dir / \"supplier_case.csv\"\n",
    "\n",
    "with psycopg2.connect(**PG) as pgconn, pgconn.cursor() as cur, open(csv_out, \"w\", encoding=\"utf-8\") as f:\n",
    "    cur.copy_expert(\"COPY (SELECT * FROM supplier_case) TO STDOUT WITH CSV HEADER\", f)\n",
    "print(\"✅ 已导出 CSV：\", csv_out.as_posix())\n",
    "\n",
    "# 读 CSV（用于建表与视图）\n",
    "df = pd.read_csv(csv_out)\n",
    "print(\"CSV 行数：\", len(df))\n",
    "print(\"CSV 列：\", list(df.columns))\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 6) 连接 Snowflake（新建独立连接，避免与 PG 混淆）\n",
    "# ---------------------------------------------------\n",
    "sf_conn = sf.connect(\n",
    "    user=os.getenv(\"SNOW_USER\",\"Ethanan2000\"),\n",
    "    password=os.getenv(\"SNOW_PASSWORD\",\"An67087833@123\"),\n",
    "    account=os.getenv(\"SNOW_ACCOUNT\",\"svogymj-bxb71103\"),\n",
    "    role=os.getenv(\"SNOW_ROLE\",\"ACCOUNTADMIN\"),\n",
    "    warehouse=os.getenv(\"SNOW_WH\",\"ETL_WH\"),\n",
    "    database=os.getenv(\"SNOW_DB\",\"ETL_DB\"),\n",
    "    schema=os.getenv(\"SNOW_SCHEMA\",\"ETL_SCHEMA\"),\n",
    "    client_session_keep_alive=True,\n",
    ")\n",
    "DB = os.getenv(\"SNOW_DB\",\"ETL_DB\")\n",
    "SC = os.getenv(\"SNOW_SCHEMA\",\"ETL_SCHEMA\")\n",
    "\n",
    "with sf_conn.cursor() as cs:\n",
    "    cs.execute(\"SELECT CURRENT_VERSION()\")\n",
    "    print(\"✅ Snowflake version:\", cs.fetchone()[0])\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 7) 推断列类型 + 识别 SupplierID/邮编列（使用你的真实列名）\n",
    "# ---------------------------------------------------\n",
    "tmap = {\n",
    "    \"int64\": \"NUMBER\",\n",
    "    \"float64\": \"FLOAT\",\n",
    "    \"object\": \"STRING\",\n",
    "    \"bool\": \"BOOLEAN\",\n",
    "    \"datetime64[ms]\": \"TIMESTAMP_NTZ\",\n",
    "    \"datetime64[ns]\": \"TIMESTAMP_NTZ\",\n",
    "}\n",
    "cols_def = \",\\n  \".join([f'\"{c}\" {tmap.get(str(t), \"STRING\")}' for c,t in df.dtypes.items()])\n",
    "\n",
    "# 你的 CSV 确认有这两列：\n",
    "sup_id_col = next((c for c in df.columns if c.lower()==\"supplierid\"), None)\n",
    "zip_col     = next((c for c in df.columns if c.lower() in {\n",
    "    \"postalpostalcode\",\"postal_code\",\"postalcode\",\"zip\",\"zipcode\",\"zip_code\",\n",
    "    \"supplierpostalcode\",\"postalcityid\"\n",
    "}), None)\n",
    "\n",
    "if not sup_id_col:\n",
    "    raise RuntimeError(f\"CSV 中缺少 SupplierID 列。当前列={list(df.columns)}\")\n",
    "if not zip_col:\n",
    "    raise RuntimeError(f\"CSV 中无法识别邮编列。当前列={list(df.columns)}\")\n",
    "\n",
    "print(f\"识别列映射：SupplierID -> {sup_id_col} ，PostalPostalCode -> {zip_col}\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 8) STAGE / FILE FORMAT / PUT / COPY / VIEW\n",
    "# ---------------------------------------------------\n",
    "full_tbl = f\"{DB}.{SC}.supplier_case\"\n",
    "\n",
    "with sf_conn.cursor() as cs:\n",
    "    cs.execute(\"CREATE STAGE IF NOT EXISTS supplier_stage\")\n",
    "    cs.execute(\"\"\"\n",
    "      CREATE OR REPLACE FILE FORMAT csv_ff_sc\n",
    "      TYPE=CSV\n",
    "      FIELD_OPTIONALLY_ENCLOSED_BY='\\\"'\n",
    "      PARSE_HEADER=TRUE\n",
    "      NULL_IF=('','NULL')\n",
    "      TRIM_SPACE=TRUE\n",
    "    \"\"\")\n",
    "\n",
    "    cs.execute(f\"CREATE OR REPLACE TABLE {full_tbl} (\\n  {cols_def}\\n)\")\n",
    "\n",
    "    # PUT：路径包含空格，使用 file:/// + 引号\n",
    "    cs.execute(f\"PUT 'file:///{csv_out.as_posix()}' @supplier_stage AUTO_COMPRESS=TRUE OVERWRITE=TRUE\")\n",
    "\n",
    "    # COPY：PARSE_HEADER=TRUE 配套 MATCH_BY_COLUMN_NAME\n",
    "    cs.execute(f\"\"\"\n",
    "      COPY INTO {full_tbl}\n",
    "      FROM @supplier_stage\n",
    "      FILE_FORMAT = (FORMAT_NAME = 'csv_ff_sc')\n",
    "      MATCH_BY_COLUMN_NAME = CASE_INSENSITIVE\n",
    "      ON_ERROR = 'ABORT_STATEMENT'\n",
    "    \"\"\")\n",
    "\n",
    "    cs.execute(f\"SELECT COUNT(*) FROM {full_tbl}\")\n",
    "    print(\"✅ Snowflake.supplier_case 行数：\", cs.fetchone()[0])\n",
    "\n",
    "    # 视图里对实际列名加双引号，避免大小写问题\n",
    "    cs.execute(f\"\"\"\n",
    "      CREATE OR REPLACE VIEW {DB}.{SC}.supplier_basic AS\n",
    "      SELECT\n",
    "        \"{sup_id_col}\" AS SupplierID,\n",
    "        \"{zip_col}\"    AS PostalPostalCode\n",
    "      FROM {DB}.{SC}.supplier_case\n",
    "    \"\"\")\n",
    "    cs.execute(f\"SELECT COUNT(*) FROM {DB}.{SC}.supplier_basic\")\n",
    "    print(\"✅ supplier_basic 行数：\", cs.fetchone()[0])\n",
    "\n",
    "    cs.execute(f\"SELECT * FROM {DB}.{SC}.supplier_basic LIMIT 5\")\n",
    "    print(\"样例：\")\n",
    "    for r in cs.fetchall():\n",
    "        print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "part 2 tast 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Snowflake: 9.27.0\n",
      "✅ 已 PUT Gazetteer 到 @weather_stage\n"
     ]
    },
    {
     "ename": "ProgrammingError",
     "evalue": "100205 (P0000): DML operation to table PYB59698.ETL_DB.ETL_SCHEMA.ZIP_LOCATIONS failed on column GEOG with error: GeoJSON::Point: Invalid Lng/Lat pair: '799292,1.66848e+08'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 57\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sf_conn\u001b[38;5;241m.\u001b[39mcursor() \u001b[38;5;28;01mas\u001b[39;00m cs:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# 原始三列：GEOID INTPTLAT INTPTLONG\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     cs\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124m      CREATE OR REPLACE TEMP TABLE \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDB\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSC\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.zip_gaz_raw AS\u001b[39m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124m      SELECT $1::string AS GEOID, $2::string AS INTPTLAT, $3::string AS INTPTLONG\u001b[39m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124m      FROM @weather_stage/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgaz_file\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (FILE_FORMAT => \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtsv_ff\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[0;32m---> 57\u001b[0m     \u001b[43mcs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;43m      CREATE OR REPLACE TABLE \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mDB\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mSC\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.zip_locations AS\u001b[39;49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;43m      SELECT\u001b[39;49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;43m        GEOID::string AS zip,\u001b[39;49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;43m        TRY_TO_DOUBLE(INTPTLAT)  AS lat,\u001b[39;49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;43m        TRY_TO_DOUBLE(INTPTLONG) AS lon,\u001b[39;49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;43m        TO_GEOGRAPHY(ST_MAKEPOINT(TRY_TO_DOUBLE(INTPTLONG), TRY_TO_DOUBLE(INTPTLAT))) AS geog\u001b[39;49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;43m      FROM \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mDB\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mSC\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.zip_gaz_raw\u001b[39;49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;43m      WHERE LENGTH(GEOID)=5\u001b[39;49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;43m        AND TRY_TO_DOUBLE(INTPTLAT)  IS NOT NULL\u001b[39;49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;43m        AND TRY_TO_DOUBLE(INTPTLONG) IS NOT NULL\u001b[39;49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     cs\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT COUNT(*) FROM \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDB\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSC\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.zip_locations\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ zip_locations 行数：\u001b[39m\u001b[38;5;124m\"\u001b[39m, cs\u001b[38;5;241m.\u001b[39mfetchone()[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/snowflake/connector/cursor.py:1134\u001b[0m, in \u001b[0;36mSnowflakeCursor.execute\u001b[0;34m(self, command, params, _bind_stage, timeout, _exec_async, _no_retry, _do_reset, _put_callback, _put_azure_callback, _put_callback_output_stream, _get_callback, _get_azure_callback, _get_callback_output_stream, _show_progress_bar, _statement_params, _is_internal, _describe_only, _no_results, _is_put_get, _raise_put_get_error, _force_put_overwrite, _skip_upload_on_content_match, file_stream, num_statements, _force_qmark_paramstyle, _dataframe_ast)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     is_integrity_error \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1131\u001b[0m         code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m100072\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1132\u001b[0m     )  \u001b[38;5;66;03m# NULL result in a non-nullable column\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m IntegrityError \u001b[38;5;28;01mif\u001b[39;00m is_integrity_error \u001b[38;5;28;01melse\u001b[39;00m ProgrammingError\n\u001b[0;32m-> 1134\u001b[0m     \u001b[43mError\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrorhandler_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/snowflake/connector/errors.py:286\u001b[0m, in \u001b[0;36mError.errorhandler_wrapper\u001b[0;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merrorhandler_wrapper\u001b[39m(\n\u001b[1;32m    265\u001b[0m     connection: SnowflakeConnection \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    268\u001b[0m     error_value: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    269\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Error handler wrapper that calls the errorhandler method.\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;124;03m        exception to the first handler in that order.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 286\u001b[0m     handed_over \u001b[38;5;241m=\u001b[39m \u001b[43mError\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhand_to_other_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m handed_over:\n\u001b[1;32m    293\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Error\u001b[38;5;241m.\u001b[39merrorhandler_make_exception(\n\u001b[1;32m    294\u001b[0m             error_class,\n\u001b[1;32m    295\u001b[0m             error_value,\n\u001b[1;32m    296\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/snowflake/connector/errors.py:341\u001b[0m, in \u001b[0;36mError.hand_to_other_handler\u001b[0;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cursor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    340\u001b[0m     cursor\u001b[38;5;241m.\u001b[39mmessages\u001b[38;5;241m.\u001b[39mappend((error_class, error_value))\n\u001b[0;32m--> 341\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrorhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/snowflake/connector/errors.py:217\u001b[0m, in \u001b[0;36mError.default_errorhandler\u001b[0;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[1;32m    215\u001b[0m errno \u001b[38;5;241m=\u001b[39m error_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merrno\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    216\u001b[0m done_format_msg \u001b[38;5;241m=\u001b[39m error_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone_format_msg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 217\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m error_class(\n\u001b[1;32m    218\u001b[0m     msg\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmsg\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    219\u001b[0m     errno\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m errno \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mint\u001b[39m(errno),\n\u001b[1;32m    220\u001b[0m     sqlstate\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqlstate\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    221\u001b[0m     sfqid\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msfqid\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    222\u001b[0m     query\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    223\u001b[0m     done_format_msg\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m done_format_msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(done_format_msg)\n\u001b[1;32m    225\u001b[0m     ),\n\u001b[1;32m    226\u001b[0m     connection\u001b[38;5;241m=\u001b[39mconnection,\n\u001b[1;32m    227\u001b[0m     cursor\u001b[38;5;241m=\u001b[39mcursor,\n\u001b[1;32m    228\u001b[0m )\n",
      "\u001b[0;31mProgrammingError\u001b[0m: 100205 (P0000): DML operation to table PYB59698.ETL_DB.ETL_SCHEMA.ZIP_LOCATIONS failed on column GEOG with error: GeoJSON::Point: Invalid Lng/Lat pair: '799292,1.66848e+08'."
     ]
    }
   ],
   "source": [
    "# ==== Task 6（Marketplace 版）：ZIP → 最近站点 → 日最高温 TMAX ====\n",
    "import os\n",
    "from pathlib import Path\n",
    "import snowflake.connector as sf\n",
    "\n",
    "# ---------- 0) Snowflake 连接 ----------\n",
    "try:\n",
    "    sf_conn\n",
    "except NameError:\n",
    "    sf_conn = sf.connect(\n",
    "        user=os.getenv(\"SNOW_USER\",\"<your_user>\"),\n",
    "        password=os.getenv(\"SNOW_PASSWORD\",\"<your_password>\"),\n",
    "        account=os.getenv(\"SNOW_ACCOUNT\",\"svogymj-bxb71103\"),\n",
    "        role=os.getenv(\"SNOW_ROLE\",\"ACCOUNTADMIN\"),\n",
    "        warehouse=os.getenv(\"SNOW_WH\",\"ETL_WH\"),\n",
    "        database=os.getenv(\"SNOW_DB\",\"ETL_DB\"),\n",
    "        schema=os.getenv(\"SNOW_SCHEMA\",\"ETL_SCHEMA\"),\n",
    "        client_session_keep_alive=True,\n",
    "    )\n",
    "\n",
    "DB = os.getenv(\"SNOW_DB\",\"ETL_DB\")\n",
    "SC = os.getenv(\"SNOW_SCHEMA\",\"ETL_SCHEMA\")\n",
    "\n",
    "IMPORTED_DB = \"WEATHER__ENVIRONMENT\"     # 你订阅到的 Imported DB\n",
    "PROVIDER_SCH = \"CYBERSYN\"                 # Provider 的 schema\n",
    "\n",
    "STATION_TBL = f\"{IMPORTED_DB}.{PROVIDER_SCH}.NOAA_WEATHER_STATION_INDEX\"\n",
    "TS_TBL      = f\"{IMPORTED_DB}.{PROVIDER_SCH}.NOAA_WEATHER_METRICS_TIMESERIES\"\n",
    "\n",
    "DATA_DIR = Path(\"/home/jovyan/MGTA SQL/final project/SQL_FINAL_PROJECT/Data-5\")\n",
    "gaz_file = DATA_DIR / \"2021_Gaz_zcta_national.txt\"\n",
    "if not gaz_file.exists():\n",
    "    raise FileNotFoundError(\"未找到 2021_Gaz_zcta_national.txt，请放到 Data-5 目录。\")\n",
    "\n",
    "with sf_conn.cursor() as cs:\n",
    "    cs.execute(\"SELECT CURRENT_VERSION()\")\n",
    "    print(\"✅ Snowflake:\", cs.fetchone()[0])\n",
    "\n",
    "# ---------- 1) Stage + File Format + PUT（Gazetteer TSV） ----------\n",
    "with sf_conn.cursor() as cs:\n",
    "    cs.execute(\"CREATE STAGE IF NOT EXISTS weather_stage\")\n",
    "    cs.execute(\"\"\"\n",
    "      CREATE OR REPLACE FILE FORMAT tsv_ff\n",
    "      TYPE=CSV FIELD_DELIMITER='\\t' SKIP_HEADER=1 NULL_IF=('','NULL') TRIM_SPACE=TRUE\n",
    "    \"\"\")\n",
    "    cs.execute(f\"PUT 'file:///{gaz_file.as_posix()}' @weather_stage OVERWRITE=TRUE AUTO_COMPRESS=TRUE\")\n",
    "    print(\"✅ 已 PUT Gazetteer 到 @weather_stage\")\n",
    "\n",
    "# ---------- 2) ZIP → 经纬度（zip_locations） ----------\n",
    "with sf_conn.cursor() as cs:\n",
    "    # 原始三列：GEOID INTPTLAT INTPTLONG\n",
    "    cs.execute(f\"\"\"\n",
    "      CREATE OR REPLACE TEMP TABLE {DB}.{SC}.zip_gaz_raw AS\n",
    "      SELECT $1::string AS GEOID, $2::string AS INTPTLAT, $3::string AS INTPTLONG\n",
    "      FROM @weather_stage/{gaz_file.name} (FILE_FORMAT => 'tsv_ff')\n",
    "    \"\"\")\n",
    "    cs.execute(f\"\"\"\n",
    "      CREATE OR REPLACE TABLE {DB}.{SC}.zip_locations AS\n",
    "      SELECT\n",
    "        GEOID::string AS zip,\n",
    "        TRY_TO_DOUBLE(INTPTLAT)  AS lat,\n",
    "        TRY_TO_DOUBLE(INTPTLONG) AS lon,\n",
    "        TO_GEOGRAPHY(ST_MAKEPOINT(TRY_TO_DOUBLE(INTPTLONG), TRY_TO_DOUBLE(INTPTLAT))) AS geog\n",
    "      FROM {DB}.{SC}.zip_gaz_raw\n",
    "      WHERE LENGTH(GEOID)=5\n",
    "        AND TRY_TO_DOUBLE(INTPTLAT)  IS NOT NULL\n",
    "        AND TRY_TO_DOUBLE(INTPTLONG) IS NOT NULL\n",
    "    \"\"\")\n",
    "    cs.execute(f\"SELECT COUNT(*) FROM {DB}.{SC}.zip_locations\")\n",
    "    print(\"✅ zip_locations 行数：\", cs.fetchone()[0])\n",
    "\n",
    "# ---------- 3) 站点索引（Marketplace） ----------\n",
    "with sf_conn.cursor() as cs:\n",
    "    # 站点索引通常含：STATION_ID / LATITUDE / LONGITUDE\n",
    "    cs.execute(f\"\"\"\n",
    "      CREATE OR REPLACE VIEW {DB}.{SC}.weather_stations AS\n",
    "      SELECT\n",
    "        station_id::string AS station_id,\n",
    "        latitude::float    AS latitude,\n",
    "        longitude::float   AS longitude,\n",
    "        TO_GEOGRAPHY(ST_MAKEPOINT(longitude::float, latitude::float)) AS geog\n",
    "      FROM {STATION_TBL}\n",
    "      WHERE latitude IS NOT NULL AND longitude IS NOT NULL\n",
    "    \"\"\")\n",
    "    cs.execute(f\"SELECT COUNT(*) FROM {DB}.{SC}.weather_stations\")\n",
    "    print(\"✅ weather_stations 行数：\", cs.fetchone()[0])\n",
    "\n",
    "# ---------- 4) 逐日 TMAX（Marketplace） ----------\n",
    "with sf_conn.cursor() as cs:\n",
    "    # Timeseries 通常含：STATION_ID / DATE / METRIC / VALUE / UNIT\n",
    "    # 这里筛选 METRIC='TMAX'，并将可能的 0.1℃ 标度标准化为 ℃（>200 则 /10）\n",
    "    cs.execute(f\"\"\"\n",
    "      CREATE OR REPLACE VIEW {DB}.{SC}.weather_tmax AS\n",
    "      SELECT\n",
    "        station_id::string                                     AS station_id,\n",
    "        TO_DATE(date)                                          AS weather_date,\n",
    "        CASE\n",
    "          WHEN value IS NULL THEN NULL\n",
    "          WHEN ABS(value::float) > 200 THEN (value::float)/10  -- NOAA 常见刻度：0.1℃ → ℃\n",
    "          ELSE value::float\n",
    "        END                                                    AS tmax_value\n",
    "      FROM {TS_TBL}\n",
    "      WHERE UPPER(metric) = 'TMAX'\n",
    "    \"\"\")\n",
    "    cs.execute(f\"SELECT COUNT(*) FROM {DB}.{SC}.weather_tmax\")\n",
    "    print(\"✅ weather_tmax 行数：\", cs.fetchone()[0])\n",
    "\n",
    "# ---------- 5) ZIP → 最近站点 ----------\n",
    "with sf_conn.cursor() as cs:\n",
    "    # supplier_basic 的 ZIP 映射到坐标\n",
    "    cs.execute(f\"\"\"\n",
    "      CREATE OR REPLACE VIEW {DB}.{SC}.supplier_zip_points AS\n",
    "      SELECT\n",
    "        s.PostalPostalCode AS zip,\n",
    "        z.lat, z.lon, z.geog\n",
    "      FROM {DB}.{SC}.supplier_basic s\n",
    "      JOIN {DB}.{SC}.zip_locations z\n",
    "        ON z.zip = s.PostalPostalCode\n",
    "    \"\"\")\n",
    "\n",
    "    # 最近站点（建议加 50km 限制；如数据稀疏可移除 WHERE）\n",
    "    cs.execute(f\"\"\"\n",
    "      CREATE OR REPLACE VIEW {DB}.{SC}.zip_nearest_station AS\n",
    "      SELECT\n",
    "        p.zip,\n",
    "        w.station_id,\n",
    "        ST_DISTANCE(p.geog, w.geog) AS distance_m\n",
    "      FROM {DB}.{SC}.supplier_zip_points p\n",
    "      JOIN {DB}.{SC}.weather_stations w\n",
    "      WHERE ST_DISTANCE(p.geog, w.geog) < 50000\n",
    "      QUALIFY ROW_NUMBER() OVER (PARTITION BY p.zip ORDER BY ST_DISTANCE(p.geog, w.geog)) = 1\n",
    "    \"\"\")\n",
    "    cs.execute(f\"SELECT COUNT(*) FROM {DB}.{SC}.zip_nearest_station\")\n",
    "    print(\"✅ zip_nearest_station 行数：\", cs.fetchone()[0])\n",
    "\n",
    "# ---------- 6) 产出最终结果 ----------\n",
    "with sf_conn.cursor() as cs:\n",
    "    cs.execute(f\"\"\"\n",
    "      CREATE OR REPLACE VIEW {DB}.{SC}.supplier_zip_code_weather AS\n",
    "      SELECT\n",
    "        s.PostalPostalCode              AS PostalPostalCode,\n",
    "        t.weather_date                  AS weather_date,\n",
    "        t.tmax_value                    AS tmax_value\n",
    "      FROM {DB}.{SC}.supplier_basic s\n",
    "      JOIN {DB}.{SC}.zip_nearest_station z\n",
    "        ON z.zip = s.PostalPostalCode\n",
    "      JOIN {DB}.{SC}.weather_tmax t\n",
    "        ON t.station_id = z.station_id\n",
    "      WHERE t.weather_date IS NOT NULL\n",
    "    \"\"\")\n",
    "    cs.execute(f\"SELECT COUNT(*) FROM {DB}.{SC}.supplier_zip_code_weather\")\n",
    "    print(\"✅ supplier_zip_code_weather 行数：\", cs.fetchone()[0])\n",
    "\n",
    "    cs.execute(f\"\"\"\n",
    "      SELECT * FROM {DB}.{SC}.supplier_zip_code_weather\n",
    "      ORDER BY PostalPostalCode, weather_date\n",
    "      LIMIT 5\n",
    "    \"\"\")\n",
    "    print(\"样例：\")\n",
    "    for r in cs.fetchall():\n",
    "        print(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
