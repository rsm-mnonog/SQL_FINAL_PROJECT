{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: snowflake-connector-python in /opt/conda/lib/python3.12/site-packages (3.17.3)\n",
      "Requirement already satisfied: asn1crypto<2.0.0,>0.24.0 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (1.5.1)\n",
      "Requirement already satisfied: boto3>=1.24 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (1.40.25)\n",
      "Requirement already satisfied: botocore>=1.24 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (1.40.25)\n",
      "Requirement already satisfied: cffi<2.0.0,>=1.9 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (1.17.1)\n",
      "Requirement already satisfied: cryptography>=3.1.0 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (45.0.4)\n",
      "Requirement already satisfied: pyOpenSSL<26.0.0,>=22.0.0 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (25.1.0)\n",
      "Requirement already satisfied: pyjwt<3.0.0 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (2.10.1)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (2025.2)\n",
      "Requirement already satisfied: requests<3.0.0 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (2.32.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (25.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (2025.6.15)\n",
      "Requirement already satisfied: typing_extensions<5,>=4.3 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (4.14.0)\n",
      "Requirement already satisfied: filelock<4,>=3.5 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (3.19.1)\n",
      "Requirement already satisfied: sortedcontainers>=2.4.0 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (2.4.0)\n",
      "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (4.3.8)\n",
      "Requirement already satisfied: tomlkit in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python) (0.13.3)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.12/site-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python) (2.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3.0.0->snowflake-connector-python) (2.4.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from boto3>=1.24->snowflake-connector-python) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /opt/conda/lib/python3.12/site-packages (from boto3>=1.24->snowflake-connector-python) (0.13.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.12/site-packages (from botocore>=1.24->snowflake-connector-python) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.24->snowflake-connector-python) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade snowflake-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2-binary\n",
      "  Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.2.3)\n",
      "Collecting lxml\n",
      "  Downloading lxml-6.0.1-cp311-cp311-manylinux_2_26_aarch64.manylinux_2_28_aarch64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading lxml-6.0.1-cp311-cp311-manylinux_2_26_aarch64.manylinux_2_28_aarch64.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: psycopg2-binary, lxml\n",
      "Successfully installed lxml-6.0.1 psycopg2-binary-2.9.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install psycopg2-binary pandas lxml python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Warehouse and Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to Snowflakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = snowflake.connector.connect(\n",
    "    user= 'Ethanan2000',\n",
    "    password = 'An67087833@123',\n",
    "    account='svogymj-bxb71103'\n",
    "    )\n",
    "\n",
    "#https://cejmwpt-djb91267.snowflakecomputing.com\n",
    "\n",
    "##user= 'mnonog',\n",
    "##    password = 'KayaNatinToL0RD!',\n",
    "##    account='cejmwpt-djb91267'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Task 1 and 2: Creating Single Table of Purchases Data and Adding the POAmount Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8976/2598083006.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_8976/2598083006.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_8976/2598083006.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_8976/2598083006.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_8976/2598083006.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_8976/2598083006.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_8976/2598083006.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_8976/2598083006.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_8976/2598083006.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_8976/2598083006.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_8976/2598083006.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_8976/2598083006.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_8976/2598083006.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_8976/2598083006.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_8976/2598083006.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_8976/2598083006.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_8976/2598083006.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_8976/2598083006.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_8976/2598083006.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_8976/2598083006.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_8976/2598083006.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_8976/2598083006.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_8976/2598083006.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_8976/2598083006.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_8976/2598083006.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_8976/2598083006.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_8976/2598083006.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_8976/2598083006.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_8976/2598083006.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_8976/2598083006.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_8976/2598083006.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_8976/2598083006.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_8976/2598083006.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_8976/2598083006.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_8976/2598083006.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_8976/2598083006.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_8976/2598083006.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_8976/2598083006.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_8976/2598083006.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_8976/2598083006.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_8976/2598083006.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combine to 1：/home/jovyan/MGTA SQL/final project/SQL final project(1)/SQL_FINAL_PROJECT/Data-5/MonthlyPOData/combined_purchases.csv line=278,222\n",
      "Snowflake 行数: 278222\n",
      "('1', '1', Decimal('18.0000'), Decimal('5.5000'), datetime.date(2019, 1, 1), '2')\n",
      "('1', '2', Decimal('21.0000'), Decimal('5.5000'), datetime.date(2019, 1, 1), '2')\n",
      "('1', '3', Decimal('18.0000'), Decimal('5.5000'), datetime.date(2019, 1, 1), '2')\n",
      "('1', '2', Decimal('21.0000'), Decimal('5.5000'), datetime.date(2019, 1, 1), '2')\n",
      "('1', '1', Decimal('18.0000'), Decimal('5.5000'), datetime.date(2019, 1, 1), '2')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0xffff85f34290>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import snowflake.connector as sf\n",
    "\n",
    "\n",
    "WH, DB, SC = \"ETL_WH\", \"ETL_DB\", \"ETL_SCHEMA\"\n",
    "BASE_DIR = Path(r\"/home/jovyan/MGTA SQL/final project/SQL final project(1)/SQL_FINAL_PROJECT/Data-5/MonthlyPOData\").resolve()\n",
    "OUT_CSV  = BASE_DIR / \"combined_purchases.csv\"   # Combine all csv file to one\n",
    "\n",
    "# 2) Columns that need to be preserved and normalized to Snowflake (target column names)\n",
    "TARGET_COLS = [\n",
    "    \"PurchaseOrderID\",\n",
    "    \"PurchaseOrderLineID\",\n",
    "    \"ReceivedOuters\",\n",
    "    \"ExpectedUnitPricePerOuter\",\n",
    "    \"OrderDate\",\n",
    "    \"SupplierID\",\n",
    "]\n",
    "\n",
    "# 3) Column name alias table (try to cover all the writing methods in your file; if not enough, add a few more)\n",
    "ALIASES = {\n",
    "    \"PurchaseOrderID\":           [\"purchaseorderid\", \"purchase_order_id\", \"poid\", \"orderid\"],\n",
    "    \"PurchaseOrderLineID\":       [\"purchaseorderlineid\", \"purchase_order_line_id\", \"polineid\", \"orderlineid\"],\n",
    "    \"ReceivedOuters\":            [\"receivedouters\", \"received_outers\", \"receivedoutersqty\", \"receivedoutersquantity\", \"received_qty\"],\n",
    "    \"ExpectedUnitPricePerOuter\": [\"expectedunitpriceperouter\", \"expected_unit_price_per_outer\", \"unitpriceperouter\", \"expectedprice\", \"unitprice\"],\n",
    "    \"OrderDate\":                 [\"orderdate\", \"order_date\", \"date\", \"order_dt\"],\n",
    "    \"SupplierID\":                [\"supplierid\", \"supplier_id\", \"vendorid\", \"vendor_id\"],\n",
    "}\n",
    "\n",
    "def norm(name: str) -> str:\n",
    "    return \"\".join(ch.lower() for ch in name if ch.isalnum())\n",
    "\n",
    "def pick_and_rename_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    orig_to_norm = {c: norm(c) for c in df.columns}\n",
    "    norm_to_orig = {v: k for k, v in orig_to_norm.items()} \n",
    "\n",
    "    selected = {}\n",
    "    missing  = []\n",
    "    for tgt, alias_list in ALIASES.items():\n",
    "        found = None\n",
    "        for alias in alias_list:\n",
    "            if alias in norm_to_orig:\n",
    "                found = norm_to_orig[alias]\n",
    "                break\n",
    "        if found is None:\n",
    "            missing.append(tgt)\n",
    "        else:\n",
    "            selected[tgt] = found\n",
    "\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns：{missing}; column={list(df.columns)}\")\n",
    "\n",
    "\n",
    "    out = df[[selected[c] for c in TARGET_COLS]].copy()\n",
    "    out.columns = TARGET_COLS\n",
    "    return out\n",
    "\n",
    "frames = []\n",
    "csv_files = sorted(BASE_DIR.glob(\"*.csv\"))\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"{BASE_DIR}\")\n",
    "\n",
    "for p in csv_files:\n",
    "    df = pd.read_csv(p, dtype=str, keep_default_na=False, na_values=[\"\", \"NULL\"])\n",
    "    df_std = pick_and_rename_columns(df)\n",
    "\n",
    "\n",
    "    for c in [\"ReceivedOuters\", \"ExpectedUnitPricePerOuter\"]:\n",
    "        df_std[c] = pd.to_numeric(df_std[c], errors=\"coerce\")\n",
    "\n",
    "    df_std[\"OrderDate\"] = pd.to_datetime(df_std[\"OrderDate\"], errors=\"coerce\", infer_datetime_format=True)\n",
    "\n",
    "\n",
    "    df_std = df_std.dropna(subset=[\"PurchaseOrderID\", \"PurchaseOrderLineID\", \"OrderDate\"])\n",
    "\n",
    "    df_std[\"ReceivedOuters\"] = df_std[\"ReceivedOuters\"].fillna(0)\n",
    "    df_std[\"ExpectedUnitPricePerOuter\"] = df_std[\"ExpectedUnitPricePerOuter\"].fillna(0)\n",
    "\n",
    "    frames.append(df_std)\n",
    "\n",
    "combined = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "combined.to_csv(OUT_CSV, index=False)\n",
    "print(f\"combine to 1：{OUT_CSV} line={len(combined):,}\")\n",
    "\n",
    "conn = sf.connect(\n",
    "    user=os.getenv(\"SNOW_USER\", \"mnonog\"),\n",
    "    password=os.getenv(\"SNOW_PASSWORD\", \"KayaNatinToL0RD!\"),\n",
    "    account=os.getenv(\"SNOW_ACCOUNT\", \"cejmwpt-djb91267\") \n",
    ")\n",
    "\n",
    "cs = conn.cursor()\n",
    "\n",
    "WH, DB, SC = \"ETL_WH\", \"ETL_DB\", \"ETL_SCHEMA\"\n",
    "cs.execute(f\"CREATE WAREHOUSE IF NOT EXISTS {WH} WAREHOUSE_SIZE=SMALL AUTO_SUSPEND=60 AUTO_RESUME=TRUE\")\n",
    "cs.execute(f\"CREATE DATABASE  IF NOT EXISTS {DB}\")\n",
    "cs.execute(f\"CREATE SCHEMA    IF NOT EXISTS {DB}.{SC}\")\n",
    "cs.execute(f\"USE WAREHOUSE {WH}\")\n",
    "cs.execute(f\"USE DATABASE {DB}\")\n",
    "cs.execute(f\"USE SCHEMA {SC}\")\n",
    "\n",
    "cs.execute(\"CREATE STAGE IF NOT EXISTS purchases_stage\")\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE FILE FORMAT csv_ff\n",
    "  TYPE = CSV\n",
    "  FIELD_OPTIONALLY_ENCLOSED_BY = '\\\"'\n",
    "  PARSE_HEADER = TRUE\n",
    "  NULL_IF = ('','NULL')\n",
    "  TRIM_SPACE = TRUE\n",
    "\"\"\")\n",
    "\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE purchases_detail (\n",
    "  PurchaseOrderID           STRING,\n",
    "  PurchaseOrderLineID       STRING,\n",
    "  ReceivedOuters            NUMBER(18,4),\n",
    "  ExpectedUnitPricePerOuter NUMBER(18,4),\n",
    "  OrderDate                 DATE,\n",
    "  SupplierID                STRING\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "cs.execute(f\"PUT 'file:///{OUT_CSV.as_posix()}' @purchases_stage AUTO_COMPRESS=TRUE OVERWRITE=TRUE\")\n",
    "\n",
    "cs.execute(\"ALTER SESSION SET DATE_INPUT_FORMAT='AUTO'\")\n",
    "cs.execute(\"\"\"\n",
    "COPY INTO purchases_detail\n",
    "FROM @purchases_stage\n",
    "FILE_FORMAT=(FORMAT_NAME='csv_ff')\n",
    "MATCH_BY_COLUMN_NAME=CASE_INSENSITIVE\n",
    "ON_ERROR='ABORT_STATEMENT'\n",
    "\"\"\")\n",
    "\n",
    "# 10) 校验\n",
    "cs.execute(\"SELECT COUNT(*) FROM purchases_detail\")\n",
    "print(\"Snowflake line:\", cs.fetchone()[0])\n",
    "cs.execute(\"SELECT * FROM purchases_detail ORDER BY OrderDate, PurchaseOrderID LIMIT 5\")\n",
    "for row in cs.fetchall():\n",
    "    print(row)\n",
    "  \n",
    "    \n",
    "# line-level view\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE VIEW PurchaseOrderTotals AS\n",
    "SELECT\n",
    "  PurchaseOrderID,\n",
    "  PurchaseOrderLineID,\n",
    "  ReceivedOuters,\n",
    "  ExpectedUnitPricePerOuter,\n",
    "  OrderDate,\n",
    "  SupplierID,\n",
    "  SUM(ReceivedOuters * ExpectedUnitPricePerOuter)\n",
    "    OVER (PARTITION BY PurchaseOrderID) AS POAmount\n",
    "FROM purchases_detail\n",
    "ORDER BY PurchaseOrderID, PurchaseOrderLineID\n",
    "\"\"\")\n",
    "\n",
    "# add column (no-op if it already exists; remove OR REPLACE since Snowflake doesn't support it on columns)\n",
    "cs.execute(\"ALTER TABLE purchases_detail ADD COLUMN POAmount NUMBER(18,2)\")\n",
    "\n",
    "# backfill totals (same POAmount for all lines of the same order)\n",
    "cs.execute(\"\"\"\n",
    "UPDATE purchases_detail AS pd\n",
    "SET POAmount = t.POAmount\n",
    "FROM (\n",
    "  SELECT\n",
    "    PurchaseOrderID,\n",
    "    ROUND(SUM(ReceivedOuters * ExpectedUnitPricePerOuter), 2) AS POAmount\n",
    "  FROM purchases_detail\n",
    "  GROUP BY PurchaseOrderID\n",
    ") AS t\n",
    "WHERE pd.PurchaseOrderID = t.PurchaseOrderID\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('invoices_stage/incoming/Supplier Transactions XML.xml.gz', 72528, '5711cd5c765040111bcba91246031d9d', 'Fri, 12 Sep 2025 01:43:59 GMT')]\n",
      "supplier_invoices rows: 2438\n",
      "(134, 2, 5, 1, 4, '7290', datetime.date(2019, 1, 2), Decimal('313.50'), Decimal('47.03'), Decimal('360.53'), Decimal('0.00'), datetime.date(2019, 1, 7), True, 4, 'incoming/Supplier Transactions XML.xml.gz')\n",
      "(169, 4, 5, 2, 4, '3898', datetime.date(2019, 1, 2), Decimal('21732.00'), Decimal('3259.80'), Decimal('24991.80'), Decimal('0.00'), datetime.date(2019, 1, 7), True, 4, 'incoming/Supplier Transactions XML.xml.gz')\n",
      "(186, 5, 5, 3, 4, '616', datetime.date(2019, 1, 2), Decimal('2740.50'), Decimal('411.11'), Decimal('3151.61'), Decimal('0.00'), datetime.date(2019, 1, 7), True, 4, 'incoming/Supplier Transactions XML.xml.gz')\n",
      "(215, 7, 5, 4, 4, '3869', datetime.date(2019, 1, 2), Decimal('42481.20'), Decimal('6372.19'), Decimal('48853.39'), Decimal('0.00'), datetime.date(2019, 1, 7), True, 4, 'incoming/Supplier Transactions XML.xml.gz')\n",
      "(224, 10, 5, 5, 4, '4697', datetime.date(2019, 1, 2), Decimal('35067.50'), Decimal('5260.14'), Decimal('40327.64'), Decimal('0.00'), datetime.date(2019, 1, 7), True, 4, 'incoming/Supplier Transactions XML.xml.gz')\n"
     ]
    }
   ],
   "source": [
    "# === 3) Extract & load supplier invoice XML (one row per invoice) ===\n",
    "\n",
    "# File format + stage (idempotent)\n",
    "cs.execute(\"CREATE STAGE IF NOT EXISTS invoices_stage\")\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE FILE FORMAT xml_ff\n",
    "  TYPE = XML\n",
    "  STRIP_OUTER_ELEMENT = TRUE\n",
    "\"\"\")\n",
    "\n",
    "# Your XML file path\n",
    "XML_FILE = Path(\"/home/jovyan/MGTA SQL/final project/SQL final project(1)/SQL_FINAL_PROJECT/Data-5/Supplier Transactions XML.xml\").resolve()\n",
    "\n",
    "# Keep XML isolated in a subfolder\n",
    "cs.execute(f\"PUT 'file:///{XML_FILE.as_posix()}' @invoices_stage/incoming AUTO_COMPRESS=TRUE OVERWRITE=TRUE\")\n",
    "\n",
    "# (optional) sanity check\n",
    "cs.execute(\"LIST @invoices_stage/incoming\"); print(cs.fetchall())\n",
    "\n",
    "# 1) Land raw XML into a VARIANT table (simple COPY is required)\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE supplier_invoices_raw (\n",
    "  doc VARIANT,\n",
    "  sourcefile STRING\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "cs.execute(\"\"\"\n",
    "COPY INTO supplier_invoices_raw (doc, sourcefile)\n",
    "FROM (\n",
    "  SELECT $1, METADATA$FILENAME\n",
    "  FROM @invoices_stage/incoming (FILE_FORMAT => 'xml_ff')\n",
    ")\n",
    "ON_ERROR='ABORT_STATEMENT'\n",
    "\"\"\")\n",
    "\n",
    "# 2) Final relational table (one row = one invoice)\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE supplier_invoices (\n",
    "  SupplierTransactionID NUMBER,\n",
    "  SupplierID            NUMBER,\n",
    "  TransactionTypeID     NUMBER,\n",
    "  PurchaseOrderID       NUMBER,\n",
    "  PaymentMethodID       NUMBER,\n",
    "  SupplierInvoiceNumber STRING,\n",
    "  TransactionDate       DATE,\n",
    "  AmountExcludingTax    NUMBER(18,2),\n",
    "  TaxAmount             NUMBER(18,2),\n",
    "  TransactionAmount     NUMBER(18,2),\n",
    "  OutstandingBalance    NUMBER(18,2),\n",
    "  FinalizationDate      DATE,\n",
    "  IsFinalized           BOOLEAN,\n",
    "  LastEditedBy          NUMBER,\n",
    "  SourceFile            STRING\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "cs.execute(\"ALTER SESSION SET DATE_INPUT_FORMAT='AUTO', TIMESTAMP_INPUT_FORMAT='AUTO'\")\n",
    "\n",
    "# 3) Shred XML from RAW -> FINAL using the hinted functions\n",
    "#    (handles either <root><row>...</row></root> OR already-row-level documents)\n",
    "cs.execute(\"\"\"\n",
    "INSERT INTO supplier_invoices\n",
    "SELECT\n",
    "  TRY_TO_NUMBER(GET(XMLGET(f.value,'SupplierTransactionID'), '$')::string)              AS SupplierTransactionID,\n",
    "  TRY_TO_NUMBER(GET(XMLGET(f.value,'SupplierID'), '$')::string)                         AS SupplierID,\n",
    "  TRY_TO_NUMBER(GET(XMLGET(f.value,'TransactionTypeID'), '$')::string)                  AS TransactionTypeID,\n",
    "  TRY_TO_NUMBER(GET(XMLGET(f.value,'PurchaseOrderID'), '$')::string)                    AS PurchaseOrderID,\n",
    "  TRY_TO_NUMBER(GET(XMLGET(f.value,'PaymentMethodID'), '$')::string)                    AS PaymentMethodID,\n",
    "  XMLGET(f.value,'SupplierInvoiceNumber'):\"$\"::string                                    AS SupplierInvoiceNumber,\n",
    "  TRY_TO_DATE(XMLGET(f.value,'TransactionDate'):\"$\"::string)                            AS TransactionDate,\n",
    "  TRY_TO_NUMBER(REPLACE(XMLGET(f.value,'AmountExcludingTax'):\"$\"::string, ',', ''),38,2) AS AmountExcludingTax,\n",
    "  TRY_TO_NUMBER(REPLACE(XMLGET(f.value,'TaxAmount'):\"$\"::string, ',', ''),38,2)          AS TaxAmount,\n",
    "  TRY_TO_NUMBER(REPLACE(XMLGET(f.value,'TransactionAmount'):\"$\"::string, ',', ''),38,2)  AS TransactionAmount,\n",
    "  TRY_TO_NUMBER(REPLACE(XMLGET(f.value,'OutstandingBalance'):\"$\"::string, ',', ''),38,2) AS OutstandingBalance,\n",
    "  TRY_TO_DATE(XMLGET(f.value,'FinalizationDate'):\"$\"::string)                           AS FinalizationDate,\n",
    "  IFF(LOWER(NULLIF(XMLGET(f.value,'IsFinalized'):\"$\"::string,'')) IN ('1','true','yes'),\n",
    "      TRUE, FALSE)                                                                       AS IsFinalized,\n",
    "  TRY_TO_NUMBER(XMLGET(f.value,'LastEditedBy'):\"$\"::string)                             AS LastEditedBy,\n",
    "  r.sourcefile                                                                           AS SourceFile\n",
    "FROM supplier_invoices_raw r,\n",
    "     LATERAL FLATTEN(\n",
    "       input => IFF(IS_ARRAY(r.doc:root.row), r.doc:root.row, ARRAY_CONSTRUCT(r.doc))\n",
    "     ) f\n",
    "\"\"\")\n",
    "\n",
    "# verify\n",
    "cs.execute(\"SELECT COUNT(*) FROM supplier_invoices\")\n",
    "print(\"supplier_invoices rows:\", cs.fetchone()[0])\n",
    "cs.execute(\"SELECT * FROM supplier_invoices ORDER BY TransactionDate, SupplierTransactionID LIMIT 5\")\n",
    "for r in cs.fetchall():\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0xffff85f34290>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.execute(f\"\"\"\n",
    "CREATE OR REPLACE VIEW {DB}.{SC}.po_invoice_join_vw AS\n",
    "SELECT\n",
    "  d.PurchaseOrderID,\n",
    "  d.PurchaseOrderLineID,\n",
    "  d.SupplierID,\n",
    "  d.OrderDate,\n",
    "  d.ReceivedOuters,\n",
    "  d.ExpectedUnitPricePerOuter,\n",
    "  i.SupplierTransactionID,\n",
    "  i.SupplierInvoiceNumber,\n",
    "  i.TransactionDate   AS InvoiceDate,\n",
    "  i.TransactionAmount AS InvoiceAmount\n",
    "FROM {DB}.{SC}.purchases_detail d\n",
    "JOIN {DB}.{SC}.supplier_invoices i\n",
    "  ON TRY_TO_NUMBER(d.PurchaseOrderID) = i.PurchaseOrderID\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Materialized view not supported for this definition; creating TABLE instead. Reason: 001998 (42710): SQL compilation error:\n",
      "Object 'PURCHASE_ORDERS_AND_INVOICES' already exists as TABLE\n",
      "Created TABLE: ETL_DB.ETL_SCHEMA.purchase_orders_and_invoices\n",
      "Rows in purchase_orders_and_invoices: 2023\n",
      "(1, 2, Decimal('10659.00'), Decimal('313.50'), Decimal('-10345.50'))\n",
      "(2, 4, Decimal('738888.00'), Decimal('21732.00'), Decimal('-717156.00'))\n",
      "(3, 5, Decimal('93177.00'), Decimal('2740.50'), Decimal('-90436.50'))\n",
      "(4, 7, Decimal('1444360.80'), Decimal('42481.20'), Decimal('-1401879.60'))\n",
      "(5, 10, Decimal('1192295.00'), Decimal('35067.50'), Decimal('-1157227.50'))\n",
      "(6, 12, Decimal('187969.00'), Decimal('5528.50'), Decimal('-182440.50'))\n",
      "(7, 4, Decimal('340017.00'), Decimal('10000.50'), Decimal('-330016.50'))\n",
      "(8, 5, Decimal('22338.00'), Decimal('657.00'), Decimal('-21681.00'))\n",
      "(9, 7, Decimal('315571.00'), Decimal('9281.50'), Decimal('-306289.50'))\n",
      "(10, 10, Decimal('35275.00'), Decimal('1037.50'), Decimal('-34237.50'))\n"
     ]
    }
   ],
   "source": [
    "# === 5) purchase_orders_and_invoices (MV if possible, else TABLE) ===\n",
    "\n",
    "# 5.1 Build a PO header total at the order level (typed as NUMBER to match invoices)\n",
    "cs.execute(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {DB}.{SC}.po_header_totals_tmp AS\n",
    "SELECT\n",
    "  TRY_TO_NUMBER(PurchaseOrderID) AS PurchaseOrderID,\n",
    "  TRY_TO_NUMBER(SupplierID)      AS SupplierID,\n",
    "  MIN(OrderDate)                 AS OrderDate,\n",
    "  ROUND(SUM(ReceivedOuters * ExpectedUnitPricePerOuter), 2) AS POAmount\n",
    "FROM {DB}.{SC}.purchases_detail\n",
    "GROUP BY 1,2\n",
    "\"\"\")\n",
    "\n",
    "# 5.2 Attempt Materialized View (fallback to TABLE if MV not supported for this join)\n",
    "create_mv_sql = f\"\"\"\n",
    "CREATE OR REPLACE MATERIALIZED VIEW {DB}.{SC}.purchase_orders_and_invoices AS\n",
    "SELECT\n",
    "  p.PurchaseOrderID,\n",
    "  p.SupplierID,\n",
    "  p.OrderDate,\n",
    "  p.POAmount,\n",
    "  i.SupplierTransactionID,\n",
    "  i.SupplierInvoiceNumber,\n",
    "  i.TransactionDate      AS InvoiceDate,\n",
    "  i.AmountExcludingTax,\n",
    "  i.TransactionAmount,\n",
    "  ROUND(i.AmountExcludingTax - p.POAmount, 2) AS invoiced_vs_quoted\n",
    "FROM {DB}.{SC}.po_header_totals_tmp p\n",
    "JOIN {DB}.{SC}.supplier_invoices i\n",
    "  ON p.PurchaseOrderID = i.PurchaseOrderID\n",
    " AND p.SupplierID      = i.SupplierID\n",
    "\"\"\"\n",
    "\n",
    "create_tbl_sql = f\"\"\"\n",
    "CREATE OR REPLACE TABLE {DB}.{SC}.purchase_orders_and_invoices AS\n",
    "SELECT\n",
    "  p.PurchaseOrderID,\n",
    "  p.SupplierID,\n",
    "  p.OrderDate,\n",
    "  p.POAmount,\n",
    "  i.SupplierTransactionID,\n",
    "  i.SupplierInvoiceNumber,\n",
    "  i.TransactionDate      AS InvoiceDate,\n",
    "  i.AmountExcludingTax,\n",
    "  i.TransactionAmount,\n",
    "  ROUND(i.AmountExcludingTax - p.POAmount, 2) AS invoiced_vs_quoted\n",
    "FROM {DB}.{SC}.po_header_totals_tmp p\n",
    "JOIN {DB}.{SC}.supplier_invoices i\n",
    "  ON p.PurchaseOrderID = i.PurchaseOrderID\n",
    " AND p.SupplierID      = i.SupplierID\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    cs.execute(create_mv_sql)\n",
    "    print(\"Created MATERIALIZED VIEW:\", f\"{DB}.{SC}.purchase_orders_and_invoices\")\n",
    "except Exception as e:\n",
    "    print(\"Materialized view not supported for this definition; creating TABLE instead. Reason:\", str(e)[:140])\n",
    "    cs.execute(create_tbl_sql)\n",
    "    print(\"Created TABLE:\", f\"{DB}.{SC}.purchase_orders_and_invoices\")\n",
    "\n",
    "# 5.3 Quick sanity checks\n",
    "cs.execute(f\"SELECT COUNT(*) FROM {DB}.{SC}.purchase_orders_and_invoices\")\n",
    "print(\"Rows in purchase_orders_and_invoices:\", cs.fetchone()[0])\n",
    "\n",
    "cs.execute(f\"\"\"\n",
    "SELECT PurchaseOrderID, SupplierID, POAmount, AmountExcludingTax, invoiced_vs_quoted\n",
    "FROM {DB}.{SC}.purchase_orders_and_invoices\n",
    "ORDER BY PurchaseOrderID\n",
    "LIMIT 10\n",
    "\"\"\")\n",
    "for r in cs.fetchall():\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "connection to server at \"127.0.0.1\", port 8765 failed: FATAL:  database \"WestCoastImporters\" does not exist\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m PG_CSV \u001b[38;5;241m=\u001b[39m EXPORT_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupplier_case.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# 6a) Postgres → CSV (server-side COPY to client, no pandas DataFrame)\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mpsycopg2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPGHOST\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPGPORT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPGUSER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPGPASSWORD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdbname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPGDATABASE\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m pg_conn, pg_conn\u001b[38;5;241m.\u001b[39mcursor() \u001b[38;5;28;01mas\u001b[39;00m cur, \u001b[38;5;28mopen\u001b[39m(PG_CSV, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# If your table is in a schema, qualify it: schema_name.supplier_case\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     cur\u001b[38;5;241m.\u001b[39mcopy_expert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCOPY supplier_case TO STDOUT WITH CSV HEADER\u001b[39m\u001b[38;5;124m\"\u001b[39m, f)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExported Postgres supplier_case -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPG_CSV\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/psycopg2/__init__.py:122\u001b[0m, in \u001b[0;36mconnect\u001b[0;34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     kwasync[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124masync_\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124masync_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    121\u001b[0m dsn \u001b[38;5;241m=\u001b[39m _ext\u001b[38;5;241m.\u001b[39mmake_dsn(dsn, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 122\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconnection_factory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwasync\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cursor_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     conn\u001b[38;5;241m.\u001b[39mcursor_factory \u001b[38;5;241m=\u001b[39m cursor_factory\n",
      "\u001b[0;31mOperationalError\u001b[0m: connection to server at \"127.0.0.1\", port 8765 failed: FATAL:  database \"WestCoastImporters\" does not exist\n"
     ]
    }
   ],
   "source": [
    "# === 6) Extract supplier_case from Postgres → local CSV → Snowflake stage ===\n",
    "# Requires: pip install psycopg2-binary\n",
    "\n",
    "import psycopg2, re\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- Postgres connection (env or defaults) ----\n",
    "PGHOST = os.getenv(\"PGHOST\", \"127.0.0.1\")\n",
    "PGPORT = int(os.getenv(\"PGPORT\", \"8765\"))\n",
    "PGUSER = os.getenv(\"PGUSER\", \"jovyan\")\n",
    "PGPASSWORD = os.getenv(\"PGPASSWORD\", \"postgres\")\n",
    "PGDATABASE = os.getenv(\"PGDATABASE\", \"WestCoastImporters\")\n",
    "\n",
    "EXPORT_DIR = Path(\"/home/jovyan/MGTA SQL/final project/SQL final project(1)/SQL_FINAL_PROJECT/Data-5\").resolve()\n",
    "EXPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PG_CSV = EXPORT_DIR / \"supplier_case.csv\"\n",
    "\n",
    "# 6a) Postgres → CSV (server-side COPY to client, no pandas DataFrame)\n",
    "with psycopg2.connect(\n",
    "    host=PGHOST, port=PGPORT, user=PGUSER, password=PGPASSWORD, dbname=PGDATABASE\n",
    ") as pg_conn, pg_conn.cursor() as cur, open(PG_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    # If your table is in a schema, qualify it: schema_name.supplier_case\n",
    "    cur.copy_expert(\"COPY supplier_case TO STDOUT WITH CSV HEADER\", f)\n",
    "\n",
    "print(f\"Exported Postgres supplier_case -> {PG_CSV}\")\n",
    "\n",
    "# 6b) Push CSV into Snowflake stage (keep it tidy in a subfolder)\n",
    "cs.execute(f\"CREATE STAGE IF NOT EXISTS {DB}.{SC}.supplier_stage\")\n",
    "cs.execute(f\"PUT 'file:///{PG_CSV.as_posix()}' @{DB}.{SC}.supplier_stage/pg_export AUTO_COMPRESS=TRUE OVERWRITE=TRUE\")\n",
    "cs.execute(f\"LIST @{DB}.{SC}.supplier_stage/pg_export\")\n",
    "print(cs.fetchall())\n",
    "\n",
    "# ---- Helper: infer Snowflake column types from CSV sample ----\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "_int_re   = re.compile(r\"^[+-]?\\d+$\")\n",
    "_float_re = re.compile(r\"^[+-]?\\d*\\.\\d+$\")\n",
    "_bool_set = {\"true\",\"false\",\"t\",\"f\",\"1\",\"0\",\"yes\",\"no\"}\n",
    "\n",
    "def _looks_bool(v:str)->bool:\n",
    "    return v.lower() in _bool_set\n",
    "\n",
    "def _looks_int(v:str)->bool:\n",
    "    return _int_re.match(v) is not None\n",
    "\n",
    "def _looks_float(v:str)->bool:\n",
    "    return _float_re.match(v) is not None\n",
    "\n",
    "def _looks_date(v:str)->bool:\n",
    "    # let Snowflake parse flexibly; we only do a quick sniff\n",
    "    for fmt in (\"%Y-%m-%d\",\"%m/%d/%Y\",\"%Y/%m/%d\"):\n",
    "        try:\n",
    "            datetime.strptime(v, fmt); return True\n",
    "        except: pass\n",
    "    return False\n",
    "\n",
    "def _looks_ts(v:str)->bool:\n",
    "    for fmt in (\"%Y-%m-%d %H:%M:%S\",\"%Y-%m-%dT%H:%M:%S\",\"%m/%d/%Y %H:%M:%S\"):\n",
    "        try:\n",
    "            datetime.strptime(v, fmt); return True\n",
    "        except: pass\n",
    "    return False\n",
    "\n",
    "def generate_snowflake_ddl(csv_path: Path, fq_table: str, sample_rows:int=2000)->str:\n",
    "    with open(csv_path, newline=\"\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        cols = reader.fieldnames\n",
    "        # flags per column\n",
    "        flags = {c: {\"bool\":True,\"int\":True,\"float\":True,\"date\":True,\"ts\":True,\"scale\":0} for c in cols}\n",
    "        n=0\n",
    "        for row in reader:\n",
    "            n += 1\n",
    "            for c in cols:\n",
    "                v = (row[c] or \"\").strip()\n",
    "                if v==\"\":\n",
    "                    continue\n",
    "                if not _looks_bool(v):  flags[c][\"bool\"]=False\n",
    "                if _looks_int(v):\n",
    "                    pass\n",
    "                else:\n",
    "                    flags[c][\"int\"]=False\n",
    "                if _looks_float(v):\n",
    "                    # track decimal scale\n",
    "                    try:\n",
    "                        sc = len(v.split(\".\")[1])\n",
    "                        flags[c][\"scale\"] = max(flags[c][\"scale\"], sc)\n",
    "                    except: pass\n",
    "                else:\n",
    "                    flags[c][\"float\"]=False\n",
    "                if not _looks_date(v): flags[c][\"date\"]=False\n",
    "                if not _looks_ts(v):   flags[c][\"ts\"]=False\n",
    "            if n>=sample_rows: break\n",
    "\n",
    "    def pick_type(fl):\n",
    "        if fl[\"ts\"]:   return \"TIMESTAMP_NTZ\"\n",
    "        if fl[\"date\"]: return \"DATE\"\n",
    "        if fl[\"bool\"]: return \"BOOLEAN\"\n",
    "        if fl[\"int\"]:  return \"NUMBER(38,0)\"\n",
    "        if fl[\"float\"]:\n",
    "            sc = min(max(fl[\"scale\"], 2), 9)\n",
    "            return f\"NUMBER(38,{sc})\"\n",
    "        return \"STRING\"\n",
    "\n",
    "    # quote identifiers safely (handle spaces/mixed case)\n",
    "    def qident(name:str)->str:\n",
    "        return '\"' + name.replace('\"','\"\"') + '\"'\n",
    "\n",
    "    cols_sql = \",\\n  \".join(f\"{qident(c)} {pick_type(flags[c])}\" for c in cols)\n",
    "    return f\"CREATE OR REPLACE TABLE {fq_table} (\\n  {cols_sql}\\n)\"\n",
    "\n",
    "# 6c) Generate CREATE TABLE from CSV header + sample values\n",
    "fq_tbl = f\"{DB}.{SC}.supplier_case\"\n",
    "ddl = generate_snowflake_ddl(PG_CSV, fq_tbl, sample_rows=5000)\n",
    "print(\"DDL:\\n\", ddl)\n",
    "cs.execute(ddl)\n",
    "\n",
    "# 6d) COPY into the Snowflake table\n",
    "cs.execute(\"ALTER SESSION SET DATE_INPUT_FORMAT='AUTO', TIME_INPUT_FORMAT='AUTO', TIMESTAMP_INPUT_FORMAT='AUTO'\")\n",
    "cs.execute(f\"\"\"\n",
    "COPY INTO {fq_tbl}\n",
    "FROM @{DB}.{SC}.supplier_stage/pg_export\n",
    "FILE_FORMAT=(FORMAT_NAME='{DB}.{SC}.csv_ff')\n",
    "MATCH_BY_COLUMN_NAME=CASE_INSENSITIVE\n",
    "PATTERN='.*supplier_case\\\\.csv\\\\.gz'\n",
    "ON_ERROR='ABORT_STATEMENT'\n",
    "\"\"\")\n",
    "\n",
    "# quick check\n",
    "cs.execute(f\"SELECT COUNT(*) FROM {fq_tbl}\")\n",
    "print(\"supplier_case rows:\", cs.fetchone()[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Task 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created TABLE (MV not allowed): ETL_DB.ETL_SCHEMA.supplier_zip_code_weather\n",
      "Rows in supplier_zip_code_weather: 29595\n",
      "('22202', datetime.date(2000, 1, 1), Decimal('13.300000'))\n",
      "('22202', datetime.date(2000, 1, 2), Decimal('20.000000'))\n",
      "('22202', datetime.date(2000, 1, 3), Decimal('20.000000'))\n",
      "('22202', datetime.date(2000, 1, 4), Decimal('21.700000'))\n",
      "('22202', datetime.date(2000, 1, 5), Decimal('8.900000'))\n",
      "('22202', datetime.date(2000, 1, 6), Decimal('8.300000'))\n",
      "('22202', datetime.date(2000, 1, 7), Decimal('12.200000'))\n",
      "('22202', datetime.date(2000, 1, 8), Decimal('7.200000'))\n",
      "('22202', datetime.date(2000, 1, 9), Decimal('8.300000'))\n",
      "('22202', datetime.date(2000, 1, 10), Decimal('11.100000'))\n"
     ]
    }
   ],
   "source": [
    "# === 7) Build supplier_zip_code_weather using ONLY the two NOAA Marketplace tables ===\n",
    "# Prereqs: {DB}.{SC}.supplier_case exists and contains a postal/zip column.\n",
    "\n",
    "# 7.0 Find a ZIP/Postal column in supplier_case\n",
    "def qident(name: str) -> str:\n",
    "    return '\"' + name.replace('\"', '\"\"') + '\"'\n",
    "\n",
    "cs.execute(f\"\"\"\n",
    "SELECT column_name\n",
    "FROM {DB}.INFORMATION_SCHEMA.COLUMNS\n",
    "WHERE table_schema = '{SC}'\n",
    "  AND table_name = 'SUPPLIER_CASE'\n",
    "  AND (\n",
    "        LOWER(column_name) LIKE '%postal%'\n",
    "     OR LOWER(column_name) LIKE '%zip%'\n",
    "     OR LOWER(column_name) LIKE '%zipcode%'\n",
    "     OR LOWER(column_name) LIKE '%zip_code%'\n",
    "  )\n",
    "ORDER BY ordinal_position\n",
    "\"\"\")\n",
    "zip_candidates = [r[0] for r in cs.fetchall()]\n",
    "if not zip_candidates:\n",
    "    raise RuntimeError(\"No ZIP/Postal column found in supplier_case.\")\n",
    "SUP_ZIP_COL_Q = qident(zip_candidates[0])\n",
    "\n",
    "# 7.1 Unique supplier ZIPs normalized to 5 digits\n",
    "cs.execute(f\"\"\"\n",
    "CREATE OR REPLACE VIEW {DB}.{SC}.supplier_zip_vw AS\n",
    "SELECT DISTINCT\n",
    "  LPAD(SUBSTR(REGEXP_REPLACE(TO_VARCHAR({SUP_ZIP_COL_Q}), '[^0-9]', ''), 1, 5), 5, '0') AS zip\n",
    "FROM {DB}.{SC}.supplier_case\n",
    "WHERE {SUP_ZIP_COL_Q} IS NOT NULL AND TRIM(TO_VARCHAR({SUP_ZIP_COL_Q})) <> ''\n",
    "\"\"\")\n",
    "\n",
    "# 7.2 Station ZIP extracted from NOAA station index (use ZIP_GEO_ID or ZIP_NAME)\n",
    "#     We do NOT use any geography/centroids—only these two NOAA tables.\n",
    "cs.execute(f\"\"\"\n",
    "CREATE OR REPLACE VIEW {DB}.{SC}.station_zip_vw AS\n",
    "SELECT\n",
    "  COALESCE(\n",
    "    REGEXP_SUBSTR(ZIP_GEO_ID, '\\\\\\\\d{{5}}'),\n",
    "    REGEXP_SUBSTR(ZIP_NAME,   '\\\\\\\\d{{5}}')\n",
    "  ) AS zip,\n",
    "  NOAA_WEATHER_STATION_ID\n",
    "FROM WEATHER__ENVIRONMENT.CYBERSYN.NOAA_WEATHER_STATION_INDEX\n",
    "WHERE COALESCE(ZIP_GEO_ID, ZIP_NAME) IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "# 7.3 Candidate stations for each supplier ZIP\n",
    "cs.execute(f\"\"\"\n",
    "CREATE OR REPLACE VIEW {DB}.{SC}.zip_station_candidates_vw AS\n",
    "SELECT sz.zip AS PostalPostalCode, st.NOAA_WEATHER_STATION_ID\n",
    "FROM {DB}.{SC}.supplier_zip_vw sz\n",
    "JOIN {DB}.{SC}.station_zip_vw st\n",
    "  ON st.zip = sz.zip\n",
    "\"\"\")\n",
    "\n",
    "# 7.4 Pick ONE \"best\" station per ZIP:\n",
    "#     Define \"best\" as the station with the most TMAX observations overall (uses only TIMESERIES).\n",
    "cs.execute(f\"\"\"\n",
    "CREATE OR REPLACE VIEW {DB}.{SC}.zip_best_station_vw AS\n",
    "WITH coverage AS (\n",
    "  SELECT\n",
    "    c.PostalPostalCode,\n",
    "    c.NOAA_WEATHER_STATION_ID,\n",
    "    COUNT(*) AS obs_days\n",
    "  FROM {DB}.{SC}.zip_station_candidates_vw c\n",
    "  JOIN WEATHER__ENVIRONMENT.CYBERSYN.NOAA_WEATHER_METRICS_TIMESERIES ts\n",
    "    ON ts.noaa_weather_station_id = c.noaa_weather_station_id\n",
    "   AND (\n",
    "        UPPER(ts.variable) IN ('TMAX','TMAX_C','TMAX_F')\n",
    "     OR LOWER(ts.variable_name) LIKE '%max%temp%'\n",
    "   )\n",
    "  GROUP BY 1,2\n",
    ")\n",
    "SELECT *\n",
    "FROM coverage\n",
    "QUALIFY ROW_NUMBER() OVER (\n",
    "  PARTITION BY PostalPostalCode\n",
    "  ORDER BY obs_days DESC, NOAA_WEATHER_STATION_ID\n",
    ") = 1\n",
    "\"\"\")\n",
    "\n",
    "# 7.5 Build the final output: (zip_code, date, high_temperature)\n",
    "create_mv_sql = f\"\"\"\n",
    "CREATE OR REPLACE MATERIALIZED VIEW {DB}.{SC}.supplier_zip_code_weather AS\n",
    "SELECT\n",
    "  m.PostalPostalCode AS zip_code,\n",
    "  ts.date            AS date,\n",
    "  ts.value           AS high_temperature\n",
    "FROM {DB}.{SC}.zip_best_station_vw m\n",
    "JOIN WEATHER__ENVIRONMENT.CYBERSYN.NOAA_WEATHER_METRICS_TIMESERIES ts\n",
    "  ON ts.noaa_weather_station_id = m.noaa_weather_station_id\n",
    "WHERE\n",
    "    UPPER(ts.variable) IN ('TMAX','TMAX_C','TMAX_F')\n",
    " OR LOWER(ts.variable_name) LIKE '%max%temp%'\n",
    "\"\"\"\n",
    "\n",
    "# Fallback to TABLE if MV not allowed\n",
    "try:\n",
    "    cs.execute(create_mv_sql)\n",
    "    print(\"Created MATERIALIZED VIEW:\", f\"{DB}.{SC}.supplier_zip_code_weather\")\n",
    "except Exception as e:\n",
    "    cs.execute(create_mv_sql.replace(\"MATERIALIZED VIEW\", \"TABLE\"))\n",
    "    print(\"Created TABLE (MV not allowed):\", f\"{DB}.{SC}.supplier_zip_code_weather\")\n",
    "\n",
    "# Sanity check\n",
    "cs.execute(f\"SELECT COUNT(*) FROM {DB}.{SC}.supplier_zip_code_weather\")\n",
    "print(\"Rows in supplier_zip_code_weather:\", cs.fetchone()[0])\n",
    "\n",
    "cs.execute(f\"\"\"\n",
    "SELECT * FROM {DB}.{SC}.supplier_zip_code_weather\n",
    "ORDER BY zip_code, date\n",
    "LIMIT 10\n",
    "\"\"\")\n",
    "for r in cs.fetchall():\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Task 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined rows (with temps): 29\n",
      "(1, 2, '80125', datetime.date(2019, 1, 2), Decimal('9718.50'), Decimal('313.50'), Decimal('360.53'), Decimal('-9405.00'), Decimal('-12.200000'))\n",
      "(3, 5, '80125', datetime.date(2019, 1, 2), Decimal('84955.50'), Decimal('2740.50'), Decimal('3151.61'), Decimal('-82215.00'), Decimal('-12.200000'))\n",
      "(5, 10, '22202', datetime.date(2019, 1, 2), Decimal('1087092.50'), Decimal('35067.50'), Decimal('40327.64'), Decimal('-1052025.00'), Decimal('8.900000'))\n",
      "(8, 5, '80125', datetime.date(2019, 1, 3), Decimal('20367.00'), Decimal('657.00'), Decimal('755.56'), Decimal('-19710.00'), Decimal('-0.600000'))\n",
      "(10, 10, '22202', datetime.date(2019, 1, 3), Decimal('32162.50'), Decimal('1037.50'), Decimal('1193.13'), Decimal('-31125.00'), Decimal('10.600000'))\n",
      "(13, 5, '80125', datetime.date(2019, 1, 4), Decimal('8091.00'), Decimal('261.00'), Decimal('300.15'), Decimal('-7830.00'), Decimal('5.600000'))\n",
      "(15, 10, '22202', datetime.date(2019, 1, 4), Decimal('413075.00'), Decimal('13325.00'), Decimal('15323.75'), Decimal('-399750.00'), Decimal('10.000000'))\n",
      "(18, 5, '80125', datetime.date(2019, 1, 7), Decimal('8928.00'), Decimal('288.00'), Decimal('331.21'), Decimal('-8640.00'), Decimal('12.800000'))\n",
      "(20, 10, '22202', datetime.date(2019, 1, 7), Decimal('224750.00'), Decimal('7250.00'), Decimal('8337.50'), Decimal('-217500.00'), Decimal('5.000000'))\n",
      "(22, 5, '80125', datetime.date(2019, 1, 7), Decimal('4882.50'), Decimal('157.50'), Decimal('181.13'), Decimal('-4725.00'), Decimal('12.800000'))\n"
     ]
    }
   ],
   "source": [
    "# === 8) Join PO/invoices + supplier_case + weather (keep only rows with temps) ===\n",
    "\n",
    "# 8.0 Find ZIP and SupplierID columns in supplier_case\n",
    "def qident(name: str) -> str:\n",
    "    return '\"' + name.replace('\"', '\"\"') + '\"'\n",
    "\n",
    "# ZIP\n",
    "cs.execute(f\"\"\"\n",
    "SELECT column_name\n",
    "FROM {DB}.INFORMATION_SCHEMA.COLUMNS\n",
    "WHERE table_schema = '{SC}'\n",
    "  AND table_name   = 'SUPPLIER_CASE'\n",
    "  AND (LOWER(column_name) LIKE '%postal%' OR LOWER(column_name) LIKE '%zip%')\n",
    "ORDER BY ordinal_position\n",
    "\"\"\")\n",
    "zip_candidates = [r[0] for r in cs.fetchall()]\n",
    "if not zip_candidates:\n",
    "    raise RuntimeError(\"Couldn't find a ZIP/Postal column in supplier_case.\")\n",
    "ZIP_COL_Q = qident(zip_candidates[0])\n",
    "\n",
    "# SupplierID\n",
    "cs.execute(f\"\"\"\n",
    "SELECT column_name\n",
    "FROM {DB}.INFORMATION_SCHEMA.COLUMNS\n",
    "WHERE table_schema = '{SC}'\n",
    "  AND table_name   = 'SUPPLIER_CASE'\n",
    "  AND (LOWER(column_name) IN ('supplierid','supplier_id') \n",
    "       OR LOWER(column_name) LIKE '%supplier%id%')\n",
    "ORDER BY ordinal_position\n",
    "\"\"\")\n",
    "sid_candidates = [r[0] for r in cs.fetchall()]\n",
    "if not sid_candidates:\n",
    "    raise RuntimeError(\"Couldn't find a SupplierID column in supplier_case.\")\n",
    "SUPPLIER_ID_COL_Q = qident(sid_candidates[0])\n",
    "\n",
    "# 8.1 Normalize supplier ZIPs and SupplierID\n",
    "cs.execute(f\"\"\"\n",
    "CREATE OR REPLACE VIEW {DB}.{SC}.supplier_zip_norm_vw AS\n",
    "SELECT\n",
    "  TRY_TO_NUMBER({SUPPLIER_ID_COL_Q}) AS SupplierID_num,\n",
    "  LPAD(SUBSTR(REGEXP_REPLACE(TO_VARCHAR({ZIP_COL_Q}), '[^0-9]', ''), 1, 5), 5, '0') AS zip5\n",
    "FROM {DB}.{SC}.supplier_case\n",
    "WHERE {SUPPLIER_ID_COL_Q} IS NOT NULL\n",
    "  AND {ZIP_COL_Q} IS NOT NULL\n",
    "  AND TRIM(TO_VARCHAR({ZIP_COL_Q})) <> ''\n",
    "\"\"\")\n",
    "\n",
    "# 8.2 Build the final joined result\n",
    "# Transaction date = InvoiceDate from purchase_orders_and_invoices\n",
    "cs.execute(f\"\"\"\n",
    "CREATE OR REPLACE VIEW {DB}.{SC}.po_inv_supplier_weather_vw AS\n",
    "SELECT\n",
    "  p.PurchaseOrderID,\n",
    "  p.SupplierID,\n",
    "  n.zip5                        AS SupplierZip,\n",
    "  p.InvoiceDate                 AS TransactionDate,\n",
    "  p.POAmount,\n",
    "  p.AmountExcludingTax,\n",
    "  p.TransactionAmount,\n",
    "  p.invoiced_vs_quoted,\n",
    "  w.high_temperature\n",
    "FROM {DB}.{SC}.purchase_orders_and_invoices p\n",
    "JOIN {DB}.{SC}.supplier_zip_norm_vw n\n",
    "  ON p.SupplierID = n.SupplierID_num\n",
    "JOIN {DB}.{SC}.supplier_zip_code_weather w\n",
    "  ON w.zip_code = n.zip5\n",
    " AND w.date     = p.InvoiceDate\n",
    "\"\"\")\n",
    "\n",
    "# quick sanity check\n",
    "cs.execute(f\"SELECT COUNT(*) FROM {DB}.{SC}.po_inv_supplier_weather_vw\")\n",
    "print(\"Joined rows (with temps):\", cs.fetchone()[0])\n",
    "\n",
    "cs.execute(f\"\"\"\n",
    "SELECT *\n",
    "FROM {DB}.{SC}.po_inv_supplier_weather_vw\n",
    "ORDER BY PurchaseOrderID, TransactionDate\n",
    "LIMIT 10\n",
    "\"\"\")\n",
    "for r in cs.fetchall():\n",
    "    print(r)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
